{
  "AI Engineer.docx": {
    "name": null,
    "emails": [
      "email@mail.com"
    ],
    "phone_numbers": [
      "(123) 456-7890"
    ],
    "education": [
      "University Texas at Dallas"
    ],
    "skills": [
      "aws",
      "azure",
      "docker",
      "git",
      "kubernetes",
      "numpy",
      "pandas",
      "postgresql",
      "python",
      "scikit-learn",
      "sql",
      "tableau",
      "tensorflow"
    ],
    "work_experiences": [
      {
        "job_title": "AI/ML Engineer",
        "company": "NerdWallet",
        "duration_months": 22.3,
        "from": "Nov 2023",
        "to": "Present",
        "is_current": true,
        "work_description": "NerdWallet, Remote. Designed advanced Generative AI (GEN AI) powered applications using Large Language Models (LLMs), NLP, Google Dialogflow, and Retrieval-Augmented Generation (RAG), improving customer interaction experiences by providing contextually relevant and accurate responses. Engineered end-to-end machine learning pipelines, integrating GPT (LLM) for natural language processing (NLP), natural language understanding (NLU), relevancy, and ranking in document retrieval, enhancing automated support and content generation workflows. Led the PoC of a scalable GPT (LLM)-based chatbot using Decoder Transformer architecture, Google Dialogflow, and LangChain, facilitating automated, human-like responses for customer support. Built and maintained real-time data pipelines using Apache Kafka and Google Cloud Pub/Sub, enabling low-latency streaming for machine learning models and improving response times for customer queries. Implemented document retrieval systems with NLP-based query expansion and semantic search, improving the relevancy and ranking of search results for customer support applications powered by Google Dialogflow for enhanced user interaction. Applied NLP techniques to analyze customer feedback and reviews, extracting key insights using sentiment analysis and topic modeling, helping product teams prioritize feature enhancements. Implemented Google AI Platform for training and deploying machine learning models, ensuring efficient handling of large datasets and automated scaling for prediction services, reducing model deployment time and enhancing relevancy in predictions. Applied RAG techniques, combining traditional machine learning and information retrieval systems, improving the ranking, relevancy, and accuracy of customer query resolution through Google Dialogflow. Collaborated with data science teams to integrate GPT-based and NLP models for content summarization, relevancy ranking, and user query resolution, significantly improving the product knowledgebase. Developed CI/CD pipelines using GitLab and Google Cloud Build to streamline the deployment of machine learning models, ensuring rapid iteration, versioning, and relevancy improvements across production environments. Enhanced model performance with distributed systems using Apache Spark on Google Cloud Dataproc, accelerating data processing times and enabling real-time model inference with accurate ranking. Collaborated with DevOps and SRE teams to ensure high availability and monitoring of AI systems, optimizing infrastructure using Google Kubernetes Engine (GKE) for orchestration and deployment. Integrated Google Identity Platform for secure, scalable user authentication in AI-driven customer support platforms using Google Dialogflow, enhancing data privacy and secure access to sensitive data. Environment: LLMs, RAG, Google Dialogflow, Hugging Face, LangChain, Python, Google AI Platform, Apache Kafka, Google Cloud Pub/Sub, Google Cloud Dataproc, Spark, GitLab, Google Kubernetes Engine, Docker, Google Identity Platform, ELK Stack, CI/CD"
      },
      {
        "job_title": "ML Engineer",
        "company": "Honda Motor Company",
        "duration_months": 46,
        "from": "Jan 2020",
        "to": "Nov 2023",
        "is_current": false,
        "work_description": "Honda Motor Company, Remote. Architected and implemented a scalable real-time data pipeline using AWS Kinesis, Lambda, and S3 to process and onboard telemetry data for real-time analytics, ensuring relevancy of streaming data analysis. Developed data onboarding and processing workflows leveraging Managed Streaming for Apache Kafka (MSK) and PySpark to stream, enrich, and transform vehicle telemetry data, integrating NLP algorithms for automated text extraction and ranking of insights based on data importance. Architected the cloud infrastructure using AWS CloudFormation (IAC) to automate the provisioning of resources like S3, EC2, RDS, Redshift, etc., optimizing for high relevancy in data flow and processing. Integrated Snowflake as the central data warehouse, optimizing storage and query performance through partitioned tables, SQL-based transformations, and NLP-driven data categorization, ensuring faster retrieval and ranking of critical data. Collaborated with data scientists to design and deploy machine learning models using Python, PySpark, and SageMaker, applying ranking algorithms to prioritize vehicle component failure predictions and trigger proactive maintenance based on high-relevancy alerts. Applied AWS Glue Data Catalog to organize and manage metadata, utilizing NLP to classify and rank data for efficient access and analysis, ensuring relevant data is prioritized in workflows. Developed and maintained large-scale data pipelines using Apache Spark, MapReduce, and PySpark, with MSK handling big data processing, ensuring that high-relevancy data is ranked, for faster ETL workflows. Implemented supervised learning techniques, including Support Vector Machines (SVM) and Decision Trees, on Amazon SageMaker, with NLP techniques applied to classify and rank incoming data. Leveraged Scikit-learn, TensorFlow, and PySpark for training machine learning models on Amazon SageMaker, including NLP models for real-time text extraction and ranking based on relevancy to vehicle performance and user behavior. Developed and deployed MLOps pipelines in production utilizing SageMaker, SageMaker-Data Wrangler, Elastic Kubernetes Service (EKS), and PySpark, incorporating NLP-driven ranking systems to enhance data quality and relevance for real-time analysis. Managed the end-to-end data pipeline lifecycle, from architecture design to deployment and optimization, ensuring high availability and reliability of PySpark-based pipelines, with ranking algorithms to prioritize data streams based on their relevancy for downstream processes. Developed custom ETL processes using AWS Glue, PySpark, and SQL to transform raw data into actionable insights, incorporating NLP and ranking models to improve downstream. Environment: AWS Kinesis, AWS Lambda, AWS S3, Apache Kafka, Snowflake, Python, AWS Glue, AWS CloudWatch, Power BI, Machine Learning, Data Pipeline, Terraform (Infrastructure as Code - IAC)"
      },
      {
        "job_title": "Data Scientist",
        "company": "Southwest Airlines",
        "duration_months": 32,
        "from": "Mar 2017",
        "to": "Nov 2019",
        "is_current": false,
        "work_description": "Southwest Airlines, Dallas, TX"
      }
    ],
    "projects": [],
    "certifications": []
  },
  "Data Scientist_1.docx": {
    "name": null,
    "emails": [
      "email@mail.com"
    ],
    "phone_numbers": [
      "(123) 456-7890"
    ],
    "education": [
      "Ma California Institute of Technology",
      "MBA The American International Institute",
      "ma Australia",
      "Bachelor Myanmar",
      "ma Myanmar"
    ],
    "skills": [
      "azure",
      "docker",
      "numpy",
      "pandas",
      "python",
      "scikit-learn",
      "sql",
      "tableau"
    ],
    "work_experiences": [
      {
        "job_title": "Unknown Title",
        "company": "Azure Data Factory",
        "duration_months": 44.3,
        "from": "Jan 2022",
        "to": "Present",
        "is_current": true,
        "work_description": "Implemented machine learning algorithms to enhance marketing by segmenting potential buyers or tenants based on preferences and behavior patterns. Developed data ingestion pipelines using Azure Data Factory to reliably extract, transform, and load geospatial data into Azure Data Lake Storage Gen 2. Utilized Databricks for preprocessing and transforming geospatial data with Python and SQL, ensuring high-quality datasets for analysis. Designed and executed spatial data analyses and visualizations using Python GIS libraries, providing actionable geographic insights. Optimized data storage and partitioning in Azure Data Lake Storage Gen 2 to improve performance and reduce latency for geospatial datasets. Enhanced data processing efficiency by integrating automated ETL workflows between Databricks, Azure Data Factory, and Azure Data Lake."
      },
      {
        "job_title": "Unknown Title",
        "company": "Azure Data Factory",
        "duration_months": 33,
        "from": "Feb 2019",
        "to": "Nov 2021",
        "is_current": false,
        "work_description": "Designed and developed an AI-powered chatbot using large language models on Azure to enhance customer service, tailored for financial advisory needs. Utilized Azure Machine Learning and Python to build and optimize chatbot machine learning models, integrating natural language processing techniques. Deployed and fine-tuned the chatbot on Azure Cloud, leveraging Azure Cognitive Services for scalability and enhanced performance. Analyzed and optimized AI orchestration in Python, reducing dependencies and improving chatbot performance. Integrated NoSQL databases to manage the chatbot’s knowledge base, enhancing the accuracy and relevance of the financial advice provided. Collaborated with cross-functional teams and monitored chatbot performance, using insights to continuously refine and align the chatbot’s functionalities with business goals."
      },
      {
        "job_title": "Unknown Title",
        "company": "Data Scientist",
        "duration_months": 29,
        "from": "Aug 2016",
        "to": "Jan 2019",
        "is_current": false,
        "work_description": "Leveraged machine learning to refine the loyalty program by identifying customer segments based on preferences and spending patterns using unsupervised clustering algorithms like K-means. Conducted extensive data analysis with Python and libraries like scikit-learn, NumPy, and pandas to derive actionable insights from customer data. Designed personalized marketing campaigns using segmentation insights to tailor promotions and rewards, significantly increasing customer engagement and loyalty. Collaborated with marketing and product teams to integrate segmentation insights into broader business strategies, aligning with corporate goals and enhancing customer-centric initiatives. Utilized Docker to standardize the machine learning environment across teams, facilitating consistent testing and development of the loyalty program. Achieved a 25% increase in customer engagement through targeted promotions, documented processes, and continuously refined the model, enhancing the program’s scalability and supporting business growth."
      },
      {
        "job_title": "Unknown Title",
        "company": "Amazon Prime",
        "duration_months": 30,
        "from": "Jan 2014",
        "to": "Jul 2016",
        "is_current": false,
        "work_description": "Conducted exploratory data analysis (EDA) with Python to identify trends and anomalies in membership data, providing insights into growth and retention strategies. Utilized SQL for data manipulation and analysis, ensuring quality and consistency across large datasets from various sources. Developed predictive models for membership growth using machine learning techniques in Python, focusing on model accuracy and performance. Employed causal inference methods to assess the impact of marketing campaigns on membership sign-ups, offering data-driven optimization recommendations. Created complex systems models to simulate membership growth scenarios, aiding stakeholders in understanding potential outcomes for informed decision-making. Increased membership growth by 20% through targeted marketing strategies and campaigns, leveraging advanced analytical insights and interactive data visualizations in Tableau for executive decision-making."
      },
      {
        "job_title": "Unknown Title",
        "company": "Software Engineer",
        "duration_months": 25,
        "from": "May 2013",
        "to": "Jun 2015",
        "is_current": false,
        "work_description": "Engineered a scalable Python API to fetch and deliver real-time currency conversion rates from multiple financial data providers, focusing on high availability and minimal latency. Implemented cloud infrastructure solutions to optimize the API's scalability and reliability, accommodating high-volume global requests. Designed a subscription feature within the API for rate alerts, enabling notifications via webhooks or emails when specific currency thresholds are reached. Enhanced user engagement by 40% by introducing features for multi-currency conversion and access to historical exchange rate data. Developed API endpoints for historical data retrieval, allowing comprehensive financial analysis and trend identification in a single API call. Collaborated with financial analysts to continuously refine API functionalities, ensuring alignment with user needs and market demands, and provided interactive documentation to aid user efficiency."
      }
    ],
    "projects": [],
    "certifications": []
  },
  "AI ML Engineer.docx": {
    "name": null,
    "emails": [
      "email@mail.com"
    ],
    "phone_numbers": [
      "(123) 456-7890"
    ],
    "education": [
      "Master NJ",
      "Institute Technology",
      "New Jersey Institute"
    ],
    "skills": [
      "Excellent problem-solving",
      "Proficient in drift handling",
      "collaboration skills",
      "communication"
    ],
    "work_experiences": [
      {
        "job_title": "Data Scientist",
        "company": "Wells Fargo",
        "duration_months": 40.3,
        "from": "May 2022",
        "to": "Present",
        "is_current": true,
        "work_description": "Developed and deployed a generative AI-driven financial forecasting system using Python, SQL, and Azure ML, leveraging time series models such as ARIMA and Prophet for accurate market trend predictions. Implemented customer segmentation models using Python with KNN and Random Forest algorithms to categorize clients based on transaction behaviors, improving personalized marketing strategies. Built and fine-tuned Llama 2-based generative AI chatbots on Azure OpenAI using Python and ANN architectures to handle real-time customer queries and financial advice, increasing customer engagement and support efficiency. Utilized mlFlow to manage the end-to-end lifecycle of machine learning models, including tracking, versioning, and deploying predictive models for financial forecasting with KNN and ANN architectures. Integrated vector databases like Pinecone to store and retrieve high-dimensional embeddings, enabling fast, accurate responses from the generative AI chatbot and improving its adaptability to complex queries. Applied ANN and Transformer architectures for natural language processing tasks, enhancing chatbot accuracy and response time. Optimized time series models using Python for real-time forecasting of financial product demand, implementing mlFlow for model version control, and using advanced seasonal decomposition techniques. Monitored and evaluated model performance with mlFlow and Azure Machine Learning tools, continuously optimizing models with KNN and ANN based on performance metrics and business KPIs, ensuring reliable forecasting. Collaborated with cross-functional teams to integrate Python machine learning models into a scalable, cloud-based infrastructure on Azure, deploying ANN and KNN algorithms for diverse AI-powered services. Developed automated drift detection systems using Azure ML, ensuring continuous model adaptation and retraining to maintain high prediction accuracy, with mlFlow tracking for drift handling."
      },
      {
        "job_title": "Data Scientist",
        "company": "Citibank",
        "duration_months": 33,
        "from": "Jul 2019",
        "to": "Apr 2022",
        "is_current": false,
        "work_description": "Developed machine learning models using Python, employing popular libraries such as scikit-learn and pandas to automate the analysis of financial marketing content, enhancing customer engagement and improving campaign efficiency. Utilized SQL for efficient data extraction and preprocessing, ensuring the integrity and scalability of large financial datasets for training machine learning models in Python. Implemented ML models including Random Forest and KNN for customer segmentation, and leveraged Time Series Forecasting models in Python to predict key financial metrics such as demand fluctuations, customer behavior trends, and campaign performance. Designed and deployed Neural Network Architectures (ANN) and Transformers, incorporating BERT to analyze and generate financial content tailored to customer needs, enhancing personalized marketing strategies in financial services. Worked with Open Source Foundational Models like LLama2, customizing them for the financial domain, and integrated GPT models for generating financial advisory content and chatbot interactions that provide real-time, context-aware assistance to customers. Managed machine learning model life cycles using mlFlow, enabling seamless model tracking, versioning, and performance evaluation for KNN and ANN models, ensuring continuous improvements in model accuracy and deployment efficiency. Utilized Azure Machine Learning and Azure OpenAI for cloud-based ML services, optimizing model deployment, scalability, and integration with existing business systems in the cloud. Integrated Vector Databases to store and retrieve financial data in a high-dimensional space, enhancing the effectiveness of LLM/GEN-AI tools and frameworks like LangChain and Agentic for advanced query handling and content generation. Implemented performance evaluation mechanisms in mlFlow to assess model accuracy, scalability, and drift handling in both ML/DL systems and KNN/ANN models, ensuring deployed solutions remained responsive to evolving customer needs and financial trends."
      },
      {
        "job_title": "Data Analyst",
        "company": "Lowe's",
        "duration_months": 19,
        "from": "Dec 2017",
        "to": "Jul 2019",
        "is_current": false,
        "work_description": "Developed and implemented machine learning models using Python to enhance retail inventory management with a focus on demand forecasting and stock optimization, ensuring improved product availability and reducing stockouts. Leveraged advanced techniques to predict sales trends and manage inventory dynamically. Engineered automated ETL pipelines using Azure Data Factory, enabling real-time extraction, transformation, and loading (ETL) of sales and inventory data from multiple sources. This ensured data integrity and readiness for ML model training and evaluation. Applied advanced statistical techniques and machine learning algorithms, including Time Series Analysis, Random Forest, KNN, and Regression Models to enhance forecasting accuracy for inventory replenishment, improving operational efficiency and ensuring timely stock replenishment in retail settings. Developed interactive dashboards with Power BI to visualize critical KPIs related to inventory turnover, demand trends, and sales projections, supporting data-driven decision-making and optimized supply chain management in retail operations. Integrated machine learning models with retail systems using Azure Functions for serverless execution, providing real-time access to predictive insights and optimizing inventory management and supply chain operations, enhancing both operational efficiency and decision-making. Utilized Azure Machine Learning (Azure ML) for scalable, automated model training and deployment, managing the end-to-end machine learning lifecycle, from model development to real-time deployment in production environments. Ensured data security through Azure Active Directory (AAD) and Azure Key Vault, protecting sensitive retail data and ensuring compliance with industry regulations, thereby safeguarding both customer and business data. Monitored and fine-tuned model performance using Azure Monitor and Azure Application Insights, ensuring optimal resource utilization and quickly adapting to changing sales patterns and business needs, improving prediction accuracy and reliability over time. Leveraged Azure SQL Database for data storage, enabling high availability, scalability, and secure management of transactional and historical retail data, allowing efficient querying and reporting to optimize inventory. Collaborated with cross-functional teams, including data analysts, operations staff, and supply chain managers, ensuring alignment between machine learning initiatives and business goals. This collaboration optimized inventory levels and enhanced customer experience through improved product availability. Documented machine learning workflows and system architecture, providing comprehensive training and support for team members, ensuring seamless deployment and operational integration of machine learning solutions into retail operations."
      }
    ],
    "projects": [],
    "certifications": []
  },
  "AI_ML_Engineer_1 External.docx": {
    "name": "Data Scientist",
    "emails": [],
    "phone_numbers": [],
    "education": [
      "Ma to Dec",
      "bs India \t\t\t\t\t\t\t\tJan",
      "ba built sales model for various product and services bundled offering",
      "Ma Learning Engineer",
      "ms India \t\t\t\t\t\t\tFeb"
    ],
    "skills": [
      "ANOVA",
      "AUC",
      "Access",
      "Acquire",
      "Aetna",
      "Agile",
      "Analytic Development",
      "Analyzed claims using NLP",
      "Azure",
      "B test)",
      "Built",
      "Classification",
      "Cleaned",
      "Cloud Services: \tAWS",
      "Cluster Centroid Methods",
      "Clustering",
      "Dallas",
      "Dashboard Reporting",
      "Data",
      "Data Consolidation",
      "Data Science",
      "Data Scientist",
      "Data Wrangling",
      "Databases",
      "Decision Tree",
      "Decision Trees",
      "Designed",
      "Developed",
      "Developed linear models",
      "Ensemble Methods",
      "Environment:",
      "Environment: Python",
      "Excel",
      "Explanatory Data Analysis",
      "Exploratory Data Analysis",
      "Fake claims",
      "Feature Engineering",
      "Feature selection",
      "GCP",
      "GPT-3",
      "GitHub 2",
      "Gleam Technologies",
      "Gradient Descent Boosting",
      "Hadoop",
      "Hierarchical clustering",
      "Hive",
      "Hugging Face's Transformers",
      "Hyper parameter tuning",
      "Hypothesis testing",
      "Hypothesis testing (A",
      "Implemented lemmatization",
      "Irving",
      "Isolation Forests",
      "Jira",
      "Jupyter",
      "K - Nearest Neighbor",
      "K-means clustering",
      "KNN",
      "Key Contributions:",
      "L1",
      "L2 regularization",
      "LLM",
      "LSTM",
      "Label Encoding",
      "Led data preparation efforts",
      "Lemmatization",
      "Local Outlier Factor",
      "Logistic Regression",
      "MATLAB",
      "MS-Access",
      "Machine Learning",
      "Machine Learning Engineer",
      "Mat plot",
      "Matplot",
      "Matplotlib",
      "MySQL",
      "NLP Engineer",
      "NLTK",
      "Naive Bayes",
      "Naïve Bayes",
      "NoSQL",
      "NumPy",
      "Numpy",
      "Other software’s: \tPyCharm",
      "Outlook",
      "POS tagging",
      "Pandas",
      "Platform: Jupyter Notebook",
      "PostgreSQL",
      "PowerPoint",
      "Prepared visualizations",
      "Professional Experience:",
      "Programming Language: \tPython",
      "PySpark",
      "PyTorch)",
      "Python Libraries: \tPandas",
      "RMS prop",
      "RNN",
      "Random Forest",
      "Random Forest Modelling",
      "Random Forests with Adaboost",
      "Regression Analysis",
      "Reporting",
      "Responsibilities:",
      "SAS",
      "SCRUM",
      "SDLC-Agile",
      "SQL",
      "SciPy",
      "Scikit-Learn",
      "Scikit-learn",
      "Seaborn",
      "Sentiment Analysis",
      "Single Value Decomposition",
      "SpaCy",
      "Spark",
      "Statistics: \tRegression",
      "Stats",
      "Support Vector Machines (SVM)",
      "TF-IDF",
      "Tableau",
      "Techniques: Data Cleaning",
      "TensorFlow",
      "Time-Series Forecasting",
      "Tokenization",
      "Tools: Python",
      "Trend Analysis",
      "Used Pandas",
      "Used both supervised",
      "Utilized",
      "Utilized Stemming",
      "Value Labs",
      "Variable Reduction",
      "Verizon",
      "Visualized the categorical",
      "Web Technology: \tMS SQL Server",
      "XGboost",
      "aggregation",
      "bar plot using Matplotlib",
      "bias",
      "data",
      "data cleansing",
      "data mapping",
      "deep - ANN",
      "developed data ingestion",
      "enhancing customer insights",
      "ensemble models",
      "ensuring accuracy",
      "ensuring high performance",
      "ensuring optimal model",
      "ensuring scalability",
      "histogram",
      "hyper parameter tuning",
      "identifying",
      "including company deals",
      "including data cleansing",
      "lemmatization",
      "matplotlib",
      "n-grams generation",
      "one note",
      "optimizing performance",
      "predict the gross margin",
      "published dashboards on web",
      "removed high-correlation",
      "sales activities",
      "scikit-learn",
      "seaborn",
      "services bundled offering",
      "stemming",
      "stemming to clean",
      "such as (tokenization",
      "t- test",
      "tickets",
      "to forecast sales growth",
      "trading around earnings",
      "transformed",
      "users",
      "utilized BERT",
      "variable reduction",
      "variance"
    ],
    "work_experiences": [
      {
        "job_title": "Unknown Title",
        "company": "Data Science /NLP Engineer",
        "duration_months": 7,
        "from": "May 2023",
        "to": "Dec 2023",
        "is_current": false,
        "work_description": "Data Science /NLP Engineer Project Description: The project involved the development of a classification model to predict whether clinics would engage in single or multiple deals with the company. The project aimed to optimize sales strategies by analyzing various datasets, including company deals, tickets, and sales activities, to forecast sales growth and improve business outcomes. Key Contributions: Developed a Random Forest classification model with 79% accuracy to predict clinic engagement in multiple deals. Built and trained NLP models for text classification and sentiment analysis using PyTorch’s Autograd for automatic differentiation. Leveraged TensorFlow’s distributed computing capabilities to train models across multiple GPUs, improving training efficiency by 20%. Applied TensorFlow to design a neural network for classifying customer feedback, enhancing business insights. Cleaned and consolidated data from multiple sources, ensuring accuracy and consistency across datasets. Analyzed state-wise sales data to identify key regions for business expansion. Applied variable reduction techniques and removed high-correlation and high-null value variables to enhance model performance. Implemented feature importance analysis to identify significant factors influencing sales outcomes. Responsibilities: Led data preparation efforts, including data cleansing, variable reduction, and encoding of categorical variables. Conducted exploratory data analysis to uncover trends and insights that informed sales strategies. Coordinated with team members to integrate additional datasets, such as Lunch-and-Learn metrics, into the analysis. Prepared visualizations and reports to communicate findings and recommendations to stakeholders. Assisted in forecasting sales growth and recommending strategies for market expansion and customer retention. Tools: Python, Pandas, Scikit-learn, Excel"
      },
      {
        "job_title": "Value Labs | India",
        "company": "Data Scientist/NLP Engineer",
        "duration_months": 42,
        "from": "Jan 2019",
        "to": "Jul 2022",
        "is_current": false,
        "work_description": "Data Scientist/NLP Engineer Project Description: Project was focused on customer clustering based on ML and statistical modeling effort including building predictive models and generate data products to support customer classification and segmentation. Also to Develop Estimation model for various product & services bundled offering to optimize and predict the gross margin, built sales model for various product and services bundled offering Key Contributions: Conducted data analysis when necessary to determine root cause of data inconsistencies. Acquire and automate additional data sources (internal and external) as needed to improve operating efficiency or support business/product needs. Applied Dimensionality Reduction techniques like PCA, t-SNE to reduce the correlation between features and high dimensionality of the data to better use time and storage. Analyzed claims using NLP and worked on NLTK package for application development. Utilized Stemming, Lemmatization, Tokenization, TF-IDF, and Bag of Words Word2Vec to get most relevant and frequent words in the claimed documents. Developed and implemented various Machine Learning Algorithms and Statistical Modeling like Text Analytics, Sentiment Analysis, Decision Tree, Naive Bayes, and Logistic Regression for predicting the binary risk levels of applicants. Finalized the optimal model based on ROC & AUC and fine-tuned the hyper parameters of the above models using Grid Search. Employed Ensemble Learning techniques such as Random Forests and Ada Gradient Boosting to improve the model performance by 15%. Responsibilities: Visualized the categorical and continuous features via mean response, histogram and bar plot using Matplotlib and Seaborn packages. Monitored incoming data to ensure Russell Investments receives timely delivery and accurate data. Employed K-Fold cross-validation to test and verify the model accuracy. Fake claims, feedback was flagged using Recurrent Neural Networks (RNN) and Long Shot Term Memory models using Keras. Environment: Python, NumPy, Pandas, Seaborn, Matplotlib, Scikit-Learn, Tableau, PostgreSQL, PySpark, Google collab."
      },
      {
        "job_title": "Gleam Technologies | India",
        "company": "Machine Learning Engineer",
        "duration_months": 25,
        "from": "Nov 2016",
        "to": "Dec 2018",
        "is_current": false,
        "work_description": "Machine Learning Engineer Responsibilities: Used both supervised and unsupervised anomaly detection techniques such as DBSCAN, Isolation Forests, Local Outlier Factor, One-Class Support Vector Machines, and deep learning (Auto encoder) etc. Worked with time series analysis to forecast the sale of product using moving average, stationarity, autocorrelation, SARIMA, VAR, LSTM, and SARIMAX. Used LSTM to predict probability of failure at different time intervals compensating for independent variables reflecting states of wear. Optimized model performance, enhancing prediction probabilities by 20%, and balancing recall-precision to improve F1 score by 15%. Leveraged AWS services for data processing, storage, and machine learning model deployment. Applied various machine learning algorithms and statistical modelling like decision trees, text analytics, natural language processing (NLP), supervised and unsupervised, regression models, neural networks, deep learning, SVM, clustering to identify Volume of product using scikit-learn package in python and PySpark. Worked on Text Mining and Text Processing for prediction of the sentimental analysis of the customer data using BERT and the Universal sentence encoder. Developed and deployed Python modelling APIs, incorporating diverse ML techniques to predict user behaviour. This facilitated numerous marketing segmentation programs, boosting their effectiveness by 25%. Segmented the customers based on demographics, geographic, behavioural, and psychographic data using K-means Clustering. Designed and implemented end-to-end systems for Data Analytics and Automation, integrating custom visualization tools using Python and Tableau. Used AWS Machine Learning for building, training, and deploying machine learning models faster using drag-and-drop designer and automated machine learning."
      },
      {
        "job_title": "Rotech Info Systems | India",
        "company": "Data Analyst",
        "duration_months": 44,
        "from": "Feb 2013",
        "to": "Oct 2016",
        "is_current": false,
        "work_description": "Project Description: Project was on developing, executing, tracking and analyzing targeted marketing campaigns. Utilized the social media campaign management application to develop and report on complex, multi-step campaigns. Analyze campaign performance, report on key business metrics and develop insights through customer analysis. Key Contributions: Developed a statistical arbitrage strategy by applying machine learning algorithms including generalized linear models, boosted regression tress and support vector machines, tuned the hyper parameters and back-tested models using validation and out-of-sample data to build prediction models. Implemented and maintained scalable python code for daily automated data update, technical indicators generations which are used to build Ensemble models to predict the expected revenue of targeted companies Created Machine Learning tools that computes adjusted P/E values and few other custom visualizations to internally used application required for teams based on tkinter module in python Responsibilities: Provided statistical insights into semi-deviation & skewness-to-kurtosis ratio to guide vendor decisions and inferences into optimum pricing for raw material order quantities. Conducted data preparation and outlier detection using Python. Used NumPy, SciPy, Matplotlib libraries for n-dimensional representation of data and plotting graphs. Performed Clustering with historical, demographic and behavioral data as features to implement the personalized marketing that offers right product to right person at the right time on the right device. Implemented Principal Component Analysis (PCA) in feature engineering to analyze high dimensional data. Perform data manipulation, data preparation, normalization, and predictive modelling. Improve efficiency and accuracy by evaluating models in Python and R. Tools: Python, Pandas, Scikit-learn, Excel Techniques: Data Cleaning, Data Consolidation, Variable Reduction, Label Encoding, Random Forest Modelling Platform: Jupyter Notebook, Excel"
      }
    ],
    "projects": [
      {
        "title": "Key Contributions:",
        "description": ""
      },
      {
        "title": "Responsibilities:",
        "description": ""
      },
      {
        "title": "Data Science /NLP Engineer",
        "description": ""
      },
      {
        "title": "Key Contributions:",
        "description": ""
      },
      {
        "title": "Responsibilities:",
        "description": ""
      }
    ],
    "certifications": []
  },
  "AI_ML_Engineer_2 External.docx": {
    "name": null,
    "emails": [],
    "phone_numbers": [],
    "education": [
      "bs Tampa FL \t\t\t\t\t\t\tDec",
      "Ba India \t\t\t\t\t\t\tAug",
      "JNT University"
    ],
    "skills": [
      "AI Tools: MLOps",
      "Azure (AKS",
      "BERT",
      "Big Data Tools: Hadoop",
      "CD Tools: Jenkins",
      "CI",
      "Cloud Platforms: AWS (EC2",
      "Cloud Storage)",
      "Containerization",
      "Django",
      "Feature Engineering",
      "Frameworks: Flask",
      "GCP (BigQuery",
      "GPT",
      "Hive",
      "Hugging Face",
      "IDEs: Jupyter",
      "IntelliJ",
      "Java",
      "Kubernetes",
      "LLAMA",
      "Languages: Python",
      "ML",
      "ML Studio)",
      "Model Optimization",
      "NLP",
      "Operating Systems: Linux",
      "Orchestration: Docker",
      "PL",
      "PyCharm",
      "PyTorch",
      "RAG",
      "RDS)",
      "RStudio",
      "S3",
      "SQL",
      "Scala",
      "Scikit-learn",
      "Spark",
      "Spring",
      "TensorFlow",
      "Terraform",
      "Tools",
      "UNIX",
      "Version Control: GitHub",
      "Windows"
    ],
    "work_experiences": [
      {
        "job_title": "AI/ML Engineer",
        "company": "Nowalabs LLC",
        "duration_months": 21.3,
        "from": "Dec 2023",
        "to": "Present",
        "is_current": true,
        "work_description": "Developed the vision and executed a scalable AI/ML engineering platform, used by Developed a scalable AI/ML engineering platform supporting over 1,000 engineers, leveraging Python and large language models like GPT, BERT, and LLAMA to streamline the AI model development lifecycle. Spearheaded the adoption of Generative AI models in conversational systems, automating customer service workflows, and improving operational efficiency with GPT-based frameworks. Led the integration of LLMs into enterprise applications using RAG techniques, optimizing information retrieval and enhancing user experiences in AI-powered platforms. Developed a text classification model to classify customer service inquiries into categories, improving response time and resolution accuracy. Implemented CI/CD pipelines using Jenkins, Kubernetes, and Terraform, supporting continuous integration and deployment of Python-based LLM models across AWS, Azure, and GCP environments. Used in building NLP pipelines for text processing and information extraction in a customer service chatbot project, focusing on user intent detection. Knowledge of NLP techniques such as tokenization, stemming. Mentored and guided teams in best practices for LLM fine-tuning, training, and deployment, ensuring robust integration with business processes. Automated infrastructure provisioning and configuration using Terraform and Kubernetes, enabling secure, consistent deployment of AI models across multi-cloud environments."
      },
      {
        "job_title": "Unknown Title",
        "company": "Lead Architect",
        "duration_months": 99,
        "from": "Aug 2015",
        "to": "Nov 2023",
        "is_current": false,
        "work_description": "Lead Architect, AI/ML Platforms Defined the architecture and led the development of enterprise-level AI/ML platforms, enabling the automation of ML pipelines and deployment processes at scale. Led the migration from monolithic systems to microservices architecture, improving scalability and operational efficiency across cloud environments such as AWS, GCP, and Azure. Orchestrated big data pipelines using Hadoop, Spark, and HBase, integrating data ingestion, transformation, and real-time processing for AI/ML model training and deployment. Experienced with NLP Techniques for understanding human language."
      }
    ],
    "projects": [],
    "certifications": []
  },
  "AI_ML_Engineer_5 External.docx": {
    "name": null,
    "emails": [],
    "phone_numbers": [],
    "education": [
      "Master",
      "University Science and Technology",
      "Masters in Electrical Engineering from Missouri University"
    ],
    "skills": [
      "API integration",
      "Algorithms: Linear Regression",
      "Android SDK frameworks",
      "Apache Airflow",
      "Application System Analyst",
      "Async tasks",
      "BLSTM",
      "Big query",
      "BigQuery to optimize",
      "CNN",
      "Cloud: GCP (VertexAI",
      "Code end-to-end rule-based",
      "Colab",
      "Composer",
      "Conducted A",
      "Conducted training sessions",
      "DBSCAN",
      "Dallas",
      "Data Science Packages: Pandas",
      "Databases: MySQL",
      "Dataproc)",
      "Decision Tree",
      "Deep Learning prototypes",
      "Deep Neural Networks",
      "Design",
      "Develop Machine Learning",
      "Developed",
      "Docker",
      "Flask",
      "Generated clean",
      "Generated the reports",
      "Gensim",
      "Gradient Descent",
      "Grafana",
      "Hugging face",
      "Identify",
      "Indu Tech Inc",
      "Integrated multiple NLP models",
      "Java",
      "K Means++",
      "K means",
      "KNN",
      "Keras",
      "Kubeflow",
      "Kubernetes",
      "LSTM",
      "Lasso",
      "Leveraged PySpark",
      "Linux",
      "Logistic regression",
      "ML metrics",
      "ML)",
      "MLFlow",
      "Mac OS",
      "Machine Learning Engineer",
      "Mentored individual engineers",
      "Misc: Fast API",
      "MongoDB",
      "Naïve Bayes",
      "NumPy",
      "Operating System: Windows",
      "Optimize",
      "PCA",
      "PowerBI",
      "Professional Experience:",
      "Programming Languages: Python",
      "PySpark",
      "PyTorch",
      "R-CNN",
      "RNN",
      "Random Forest",
      "Refactor existing code",
      "Research",
      "Responsibilities:",
      "Responsible for designing",
      "Ridge",
      "Role: Senior AI",
      "SHAP",
      "SVM",
      "San Carlos",
      "San Francisco",
      "San Jose",
      "SciPy",
      "Scikit-learn",
      "Seaborn",
      "Shared Preferences to store",
      "Spacy",
      "System Engineer",
      "TensorFlow",
      "Transcend-CS",
      "Transfer Learning",
      "Used correlation analysis",
      "Used services",
      "Verizon",
      "Version Control: Git",
      "Visualization: Matplotlib",
      "Workbench",
      "XGBOOST",
      "a bag of words",
      "accelerating development",
      "animation",
      "automation",
      "catboost",
      "column names",
      "correlations",
      "custom buttons",
      "data",
      "data cleaning",
      "data collection",
      "enhancing customer call",
      "ensuring scalable",
      "feature engineering",
      "focusing on throughput",
      "formatting techniques",
      "generated user-friendly",
      "getting JSON response",
      "implementation of custom Uts",
      "including data preprocessing",
      "including optimizations",
      "lightgbm",
      "maintain user information",
      "metrics",
      "model deployment(AI",
      "model development",
      "model prediction",
      "model training",
      "model validation",
      "optimizations",
      "plotly",
      "potential use cases of chatGPT",
      "refined automation frameworks",
      "regularization strength",
      "reproducible model training",
      "stability",
      "statistical",
      "tf-idf",
      "track critical datapoints",
      "uniformity",
      "updated the models as new data",
      "utilizing JSON",
      "vectorization",
      "word2vec",
      "xgboost"
    ],
    "work_experiences": [
      {
        "job_title": "Unknown Title",
        "company": "Responsibilities:",
        "duration_months": 15.3,
        "from": "Jun 2024",
        "to": "Present",
        "is_current": true,
        "work_description": "Role: Senior AI/ Machine Learning Engineer Responsibilities: Developed and deployed end-to-end data pipelines using Airflow on GCP Composer for data preprocessing, model prediction, and calculating SHAP values for feature importance on batch processing systems. Built Grafana dashboards to monitor pipelines, track critical datapoints, and implemented word clouds to monitor text-based data in NLP models for better insights and performance tracking. Leveraged PySpark and BigQuery to optimize and accelerate data preprocessing workflows for large datasets. Configured the entire project using TOML files for streamlined project management and configuration. Implemented serverless deployment of AI models using GCP Cloud Functions for real-time inference, enhancing customer call and chat analytics by automating sentiment analysis and conversational insights."
      },
      {
        "job_title": "Unknown Title",
        "company": "Role: Machine Learning Engineer",
        "duration_months": 66,
        "from": "Oct 2018",
        "to": "Apr 2024",
        "is_current": false,
        "work_description": "Role: Machine Learning Engineer Responsibilities: Spearheaded cross-functional collaboration between multiple teams, accelerating development and debug cycles."
      }
    ],
    "projects": [],
    "certifications": []
  },
  "Cloud Engineer.docx": {
    "name": "Data Engineer",
    "emails": [],
    "phone_numbers": [],
    "education": [
      "University Texas at Dallas"
    ],
    "skills": [
      "AWS CloudWatch",
      "AWS CodePipeline",
      "AWS EKS",
      "AWS Glue",
      "AWS Kinesis",
      "AWS Lambda",
      "AWS Redshift",
      "AWS Redshift for storing",
      "AWS SQS",
      "AWS SageMaker",
      "Amazon Cognito",
      "Amazon Kinesis",
      "Amazon Lex",
      "Amazon RDS",
      "Amazon SNS",
      "Amazon SQS",
      "Apache Kafka",
      "Architected",
      "Automated deployment",
      "Automated the deployment",
      "Bengalore",
      "Bengaluru",
      "Built",
      "CD",
      "CD pipelines using GitHub",
      "CI",
      "CloudFormation",
      "CloudFormation (IaC)",
      "CloudTrail",
      "CloudWatch",
      "Collaborated with marketing",
      "Dallas",
      "Data Pipeline",
      "Deployed",
      "Designed",
      "Developed",
      "Developed data onboarding",
      "Docker",
      "EC2",
      "EKS to enhance data quality",
      "ETL",
      "Engineered",
      "Environment: AWS Kinesis",
      "Environment: AWS Lambda",
      "Environment: AWS S3",
      "Environment: Python",
      "Git",
      "GitHub",
      "GitHub CI",
      "Glue",
      "Honda Motor Company",
      "IND",
      "Jenkins",
      "Kubernetes",
      "Kubernetes (EKS)",
      "Lambda",
      "Leveraged Scikit-learn",
      "NerdWallet",
      "Optimized data partitioning",
      "PostgreSQL",
      "Power BI",
      "PySpark",
      "PySpark to stream",
      "Python",
      "RDS",
      "RDS)",
      "Redshift",
      "S3",
      "SNS",
      "SQL",
      "SQS",
      "SQS to process",
      "SageMaker",
      "Snowflake",
      "Southwest Airlines",
      "Streamlined batch",
      "TX",
      "TensorFlow",
      "Terraform (IAC)",
      "Torrence",
      "Used AWS Glue Data Catalog",
      "achieving high availability",
      "alerting",
      "analyze",
      "containerized deployments",
      "creating custom dashboards",
      "enabling scalable",
      "enhancing data quality",
      "enhancing performance",
      "enhancing scalability",
      "enrich telemetry data",
      "ensuring reliability",
      "ensuring seamless deployment",
      "external sources",
      "facilitating efficient scaling",
      "failure points",
      "high availability",
      "implemented an AI-powered chat",
      "improving data accessibility",
      "including retry logic",
      "manage AWS resources",
      "onboard telemetry data",
      "operational data",
      "optimizing storage",
      "perform data transformations",
      "predictive maintenance",
      "process large datasets",
      "product teams to monitor",
      "real-time processing workflows",
      "real-time transactional data",
      "reducing setup time",
      "scalability",
      "security",
      "testing",
      "transformation",
      "unstructured data",
      "utilized SNS"
    ],
    "work_experiences": [
      {
        "job_title": "Sr. Data Engineer",
        "company": "Honda Motor Company",
        "duration_months": 68.3,
        "from": "Jan 2020",
        "to": "Present",
        "is_current": true,
        "work_description": "Honda Motor Company, Torrence, CA. Architected and implemented a scalable real-time data pipeline using AWS Kinesis, Lambda, S3, and SQS to process and onboard telemetry data, ensuring efficient real-time analytics. Developed data onboarding and processing workflows leveraging Managed Streaming for Apache Kafka (MSK), SNS, and PySpark to stream, enrich telemetry data, achieving real-time data availability. Architected cloud infrastructure on AWS using AWS CloudFormation to automate provisioning of resources like S3, EC2, RDS, Redshift, SNS, and SQS, streamlining resource management. Integrated Snowflake as the central data warehouse, optimizing storage and query performance through partitioned tables and SQL-based transformations for advanced analytics. Collaborated with data scientists to design and deploy machine learning models in Kubernetes and SageMaker, enabling scalable, containerized deployments and model predictions oncomponent failures. Used AWS Glue Data Catalog and SageMaker-Data Wrangler to manage metadata, process large datasets, and perform data transformations, facilitating efficient data monitoring and analysis. Developed and managed large-scale data pipelines using MapReduce, PySpark, and MSK for big data processing in ETL workflows, incorporating SQS to manage message queuing across processes. Automated deployment and management of data pipeline components using AWS CloudFormation (IaC), Kubernetes, and GitHub CI/CD ensuring environment consistency and scaling capabilities. Leveraged Scikit-learn, TensorFlow, and PySpark to train machine learning models on SageMaker, facilitating scalable model deployment and efficient processing of telemetry and transaction data. Designed and delivered Power BI dashboards for insights into vehicle performance, failure points, and predictive maintenance, supporting data-driven decisions. Collaborated with DevOps to establish CI/CD pipelines using GitHub, Jenkins, Kubernetes, and AWS CodePipeline, supporting efficient deployment and version control for data infrastructure. Deployed and maintained MLOps pipelines in production with SageMaker-Data Wrangler and Elastic Kubernetes Service (EKS), enhancing data quality and enabling model retraining. Managed end-to-end data pipeline lifecycle from design to deployment, ensuring reliability, scalability, and high availability for PySpark-based pipelines. Created custom ETL processes with AWS Glue, PySpark, SQS, and SQL to transform raw data into actionable insights, optimizing data for advanced analytics and machine learning. Environment: AWS Kinesis, AWS Lambda, S3, Glue, CloudWatch, SQS, SNS, RDS, Redshift, Kubernetes, Apache Kafka, Snowflake, Python, PySpark, Power BI, GitHub, CI/CD, CloudFormation (IaC)"
      },
      {
        "job_title": "Data Science Engineer",
        "company": "Southwest Airlines",
        "duration_months": 22,
        "from": "Aug 2016",
        "to": "Jun 2018",
        "is_current": false,
        "work_description": "Southwest Airlines, Dallas, TX Designed and implemented scalable data pipelines using AWS Glue and PySpark, automating ETL processes for handling large volumes of structured and unstructured data, integrating them into Snowflake. Developed Infrastructure-as-Code (IaC) using CloudFormation to provision and manage AWS resources, ensuring consistency across environments, enhancing scalability, and reducing configuration errors. Built real-time data streaming applications with AWS Kinesis, MSK (Managed Streaming for Apache Kafka), and AWS SQS, processing millions of events per second for live analytics and message queuing. Led the migration of legacy on-premises data pipelines to the AWS Cloud using CloudFormation (IaC), achieving high availability, security, and scalability across multiple environments. Leveraged Kubernetes for managing distributed containers, ensuring efficient resource management and high availability, and utilized SNS and SQS for decoupled communication within architecture. Developed and managed data workflows with PySpark, leveraging Git for version control and GitHub CI/CD pipelines for streamlined operations across development, testing, and production environments. Collaborated with DevOps to implement CI/CD pipelines using GitHub, Jenkins, and AWS CodePipeline, ensuring seamless deployment and version control for infrastructure and data pipelines. Implemented real-time monitoring and anomaly detection with AWS CloudWatch, Amazon SNS, and Amazon SQS, creating custom dashboards and setting up alerts for proactive system health monitoring. Deployed containerized applications using Docker and managed orchestration through Kubernetes (EKS), facilitating efficient scaling and resource allocation for data pipelines, enhancing fault tolerance. Built and optimized data lakes on Amazon S3 and AWS Redshift for storing and analyzing terabytes of historical and real-time transactional data, improving data accessibility and scalability. Enhanced data pipeline resilience and scalability through PySpark-based batch and real-time processing workflows, optimizing performance for distributed cloud infrastructure. Streamlined batch and real-time data processing using PySpark to optimize distributed data pipelines in the cloud, enhancing performance and reducing latency. Environment: Python, SQL, PostgreSQL, Snowflake, AWS Glue, CloudFormation, AWS Kinesis, Docker, Kubernetes (EKS), AWS CloudWatch, SNS, SQS, S3, Redshift, Git, GitHub, CI/CD, Jenkins, PySpark."
      },
      {
        "job_title": "AWS Data Engineer",
        "company": "Berkshire Hathaway Homestate Companies (BHHC)",
        "duration_months": 30,
        "from": "Jun 2018",
        "to": "Dec 2020",
        "is_current": false,
        "work_description": "Berkshire Hathaway Homestate Companies (BHHC), Bengalore, IND Architected the cloud infrastructure on AWS using AWS CloudFormation (IAC) to automate the provisioning of resources like S3, EC2, RDS, Redshift, etc. Architected the end-to-end data flow from AWS S3 to Snowflake, ensuring efficient data onboarding, transformation, and loading processes while minimizing latency. Developed custom ETL processes using AWS Glue and SQL to transform raw data into actionable insights, optimizing data for downstream analytics. Collaborated with DevOps to implement CI/CD pipelines using GitHub, Jenkins, and AWS CodePipeline, ensuring seamless deployment and version control for infrastructure and data pipelines. Implemented robust error handling mechanisms, including retry logic and alerting, to ensure data pipeline reliability and quick resolution of data processing failures. Automated the deployment and management of data pipeline components using AWS CloudFormation (IAC), reducing setup time and ensuring consistency across cloud platforms. Integrated AWS Lambda functions to trigger real-time data processing workflows based on specific events, improving data timeliness for business-critical operations. Conducted performance tuning of SQL queries and Snowflake data warehouse configurations to enhance query speed and reduce processing costs. Optimized data partitioning and storage strategies in Snowflake, ensuring the pipeline could efficiently handle varying data volumes without performance degradation. Environment: AWS Lambda, AWS Glue, Redshift, Snowflake, SQL, CloudWatch, ETL, Data Pipeline, CloudTrail, CloudFormation, Terraform (IAC)"
      },
      {
        "job_title": "Data Engineer",
        "company": "NerdWallet",
        "duration_months": 23,
        "from": "Aug 2014",
        "to": "Jul 2016",
        "is_current": false,
        "work_description": "NerdWallet, Bengaluru, IND Designed and optimized end-to-end ETL pipelines using AWS services (S3, Glue, Redshift, RDS), ensuring automated T-1 data extraction, transformation, and loading of data. Engineered and implemented an AI-powered chat/voice bot using Amazon Lex to replace the legacy IVR system, automating customer support processes and leading to a reduction in manual intervention. Conducted extensive data analysis on customer support interactions, leveraging machine learning to extract actionable insights and inform product update strategies. Enhanced ETL pipelines to extract data form Salesforce email campaign data, operational data, and external sources, achieving increase in performance efficiency. Developed and deployed machine learning models in production utilizing AWS SageMaker and EKS to enhance data quality, achieving an improvement in overall data utilization. Developed live dashboards for real-time issue tracking using Power BI, enabling the software development and product teams to monitor, analyze, and address the most frequent customer complaints. Collaborated with marketing and customer support teams to implement targeted retention strategies based on churn predictions, directly contributing to improvement in customer satisfaction. Established more secure customer profiles using Amazon Cognito, further improving customer privacy. Environment: AWS S3, AWS Glue, AWS Redshift, Amazon RDS, Amazon Lex, Amazon Kinesis, AWS SageMaker, AWS EKS, Python, Power BI, ETL, Amazon Cognito University of Texas at Dallas - Master of Science in Business Analytics; Graduate Certificate: Specialization in Applied Machine Learning."
      }
    ],
    "projects": [],
    "certifications": []
  },
  "Data Engineer.docx": {
    "name": "Data Engineer",
    "emails": [
      "email@mail.com"
    ],
    "phone_numbers": [
      "(123) 456-7890"
    ],
    "education": [
      "University Texas at Dallas"
    ],
    "skills": [
      "aws",
      "docker",
      "excel",
      "kubernetes",
      "python",
      "sql",
      "tableau"
    ],
    "work_experiences": [
      {
        "job_title": "Sr. Data Engineer",
        "company": "Honda Motor Company",
        "duration_months": 39.3,
        "from": "Jun 2022",
        "to": "Present",
        "is_current": true,
        "work_description": "Honda Motor Company, Remote. Architected and implemented a scalable real-time data pipeline using AWS Kinesis, Lambda, and S3 to process telemetry data. Developed data ingestion and processing workflows leveraging Apache Kafka to stream and enrich vehicle telemetry data, ensuring real-time data availability. Integrated Snowflake as the central data warehouse, optimizing storage and query performance through partitioned tables for advanced analytics. Collaborated with data scientists to design and deploy machine learning models in Python that predict vehicle component failures, triggering proactive maintenance alerts. Established monitoring and log systems using AWS CloudWatch and Datadog, optimizing data flow to handle peak loads with consistent throughput and minimal latency. Worked with cross-functional teams, including software engineers and product managers, to define technical requirements and ensure seamless pipeline integration. Implemented data governance practices, ensuring data encryption, access control, and compliance with data privacy regulations across all telemetry data processes. Automated data pipeline workflows using AWS Glue and Lambda, reducing manual intervention and improving data freshness for real-time analytics. Set up a monitoring framework using Datadog to track data pipeline performance and flow, allowing for proactive issue detection and logging. Designed and delivered dashboards and reports in Power BI, providing insights into vehicle performance, potential failure points, and predictive maintenance effectiveness. Improved predictive maintenance accuracy, reducing unexpected vehicle breakdowns and enhancing customer satisfaction. Reduced overall maintenance costs through proactive identification and resolution of vehicle issues before they become critical. Achieved sub-second data processing latency, ensuring timely and accurate data for predictive analytics and decision-making. Managed the end-to-end data pipeline lifecycle, from initial architecture to deployment and ongoing optimization, ensuring high availability and reliability. Executed exploratory data analysis to uncover patterns and relationships in customer behavior, which guided feature selection and informed the development of more effective predictive models. Visualized prediction outcomes using Power BI, creating interactive dashboards and comprehensive reports to effectively communicate insights and predictions to key stakeholders. Delivered knowledge transfer sessions and detailed documentation to enable smooth handover of the solution to the operations and maintenance teams. Environment: AWS Kinesis, AWS Lambda, AWS S3, Apache Kafka, Snowflake, Python, AWS Glue, AWS CloudWatch, Power BI, Machine Learning, Data Pipeline, Datadog"
      },
      {
        "job_title": "AWS Data Engineer",
        "company": "Berkshire Hathaway Homestate Companies (BHHC)",
        "duration_months": 29,
        "from": "Jan 2020",
        "to": "Jun 2022",
        "is_current": false,
        "work_description": "Berkshire Hathaway Homestate Companies (BHHC), Omaha, NE Architected the cloud infrastructure on AWS using AWS CloudFormation to automate the provisioning of resources like S3, EC2, RDS, Redshift, etc. Designed and implemented scalable data pipelines that integrated AWS services (S3, RDS, Redshift, Glue) with Snowflake to enable seamless data processing and storage. Architected the end-to-end data flow from AWS S3 to Snowflake, ensuring efficient data ingestion, transformation, and loading processes while minimizing latency. Developed custom ETL processes using AWS Glue and SQL to transform raw data into actionable insights, optimizing data for downstream analytics. Performed extensive data preprocessing, including data cleaning, normalization, and feature engineering, to prepare large-scale historical data for accurate analysis and predictive modeling. Implemented robust error handling mechanisms, including retry logic and alerting, to ensure data pipeline reliability and quick resolution of data processing failures. Automated the deployment and management of data pipeline components using AWS CloudFormation, reducing setup time and ensuring consistency across environments. Integrated AWS Lambda functions to trigger real-time data processing workflows based on specific events, improving data timeliness for business-critical operations. Designed the security architecture with IAM roles and policies. Utilized AWS KMS for encryption to protect data in transit and at rest. Configured AWS Redshift as a temporary staging environment for large datasets, allowing for efficient data aggregation before final transfer to Snowflake. Conducted performance tuning of SQL queries and Snowflake data warehouse configurations to enhance query speed and reduce processing costs. Set up a monitoring framework using Datadog to track data pipeline performance and flow, allowing for proactive issue detection and logging. Optimized data partitioning and storage strategies in Snowflake, ensuring the pipeline could efficiently handle varying data volumes without performance degradation. Delivered detailed documentation and knowledge transfer sessions to ensure the smooth handover of the solution to the operations team. Achieved near-zero downtime with the implementation of robust error handling and automated recovery mechanisms. Environment: AWS Lambda, AWS Glue, Redshift, Snowflake, SQL, CloudWatch, ETL, Data Pipeline, CloudTrail, Datadog"
      },
      {
        "job_title": "Data Engineer",
        "company": "NerdWallet",
        "duration_months": 20,
        "from": "Mar 2018",
        "to": "Nov 2019",
        "is_current": false,
        "work_description": "NerdWallet, San Francisco, CA Designed and optimized end-to-end ETL pipelines using AWS services (S3, Glue, Redshift, RDS), ensuring automated T-1 data extraction, transformation, and loading of data. Engineered and implemented an AI-powered chat/voice bot using Amazon Lex to replace the legacy IVR system, automating customer support processes and leading to a 60% reduction in manual intervention. Created and managed data streaming pipelines using Amazon Kinesis, enabling the company to process and analyze customer behavior data in real time, allowing the company to respond quickly to changes in customer preferences and market trends. Conducted extensive data analysis on customer support interactions, leveraging machine learning to extract actionable insights and inform product update strategies. Enhanced ETL pipelines to extract data form Salesforce email campaign data, operational data, and external sources, achieving increase in performance efficiency. Developed and deployed machine learning models in production utilizing AWS SageMaker and EKS to enhance data quality, achieving an improvement in overall data utilization. Developed live dashboards for real-time issue tracking using Power BI, enabling the software development and product teams to monitor, analyze, and address the most frequent customer complaints. Collaborated with marketing and customer support teams to implement targeted retention strategies based on churn predictions, directly contributing to improvement in customer satisfaction. Performed extensive data preprocessing, including data cleaning, normalization, and feature engineering, to prepare large-scale historical data for accurate analysis and predictive modelling. Improved customer satisfaction by streamlining the support process, reducing response times, and providing the development team with insights to address common issues proactively. Implemented data security and privacy measures in compliance with industry regulations, ensuring data protection and governance. Established more secure customer profiles using Amazon Cognito, further improving customer privacy. Environment: AWS S3, AWS Glue, AWS Redshift, Amazon RDS, Amazon Lex, Amazon Kinesis, AWS SageMaker, AWS EKS, Python, Power BI, ETL, Amazon Cognito"
      }
    ],
    "projects": [],
    "certifications": []
  },
  "Data Scientist 2.docx": {
    "name": null,
    "emails": [
      "email@mail.com"
    ],
    "phone_numbers": [
      "(123) 456-7890"
    ],
    "education": [
      "Arizona State University"
    ],
    "skills": [
      "AWS",
      "AWS (Lambda",
      "AWS (SageMaker",
      "Atlanta",
      "Basking Ridge",
      "Bellevue",
      "CloudWatch)",
      "Created",
      "Designed",
      "Developed",
      "Docker",
      "ETL processes",
      "Employed feature selection",
      "Environments: Python",
      "Extracted",
      "Ga\t\t\t\t\t\t   08",
      "Gathered",
      "Generative AI technologies",
      "Glue",
      "Instituted regular monitoring",
      "Jupyter Notebook",
      "Keras",
      "Kubernetes",
      "Kubernetes on AWS",
      "NJ\t\t\t\t\t\t\t07",
      "NumPy",
      "PCA",
      "PROFESSIONAL EXPERIENCE:",
      "Pandas",
      "Power BI",
      "Proficient in SQL",
      "PyTorch",
      "Random Forest",
      "Redshift",
      "Responsibilities:",
      "S3 for data storage",
      "S3)",
      "SQL",
      "Scikit-Learn",
      "Senior Data Scientist",
      "T-Mobile",
      "Tableau",
      "TensorFlow",
      "WA\t05",
      "XGBoost",
      "churn trends",
      "enabling efficient querying",
      "enabling targeted marketing",
      "ensure compliance with ethical",
      "ensuring data consistency",
      "ensuring model scalability",
      "ensuring scalability",
      "including IT",
      "including bias mitigation",
      "industry benchmarks",
      "integrate structured",
      "interpreted",
      "leveraging Python",
      "machine learning algorithms",
      "manipulation",
      "patterns",
      "performance metrics",
      "predictive insights",
      "preprocess",
      "product management",
      "quality for model training",
      "reducing computational costs",
      "such as Random Forest",
      "to assess customer sentiment",
      "transformed",
      "trends",
      "user engagement trends",
      "user needs"
    ],
    "work_experiences": [
      {
        "job_title": "Senior Data Scientist",
        "company": "Responsibilities:",
        "duration_months": 24.3,
        "from": "Sep 2023",
        "to": "Present",
        "is_current": true,
        "work_description": "Responsibilities: Developed and deployed a chatbot using Python and Generative AI technologies, leveraging Large Language Models (LLMs) on AWS to enhance customer engagement and support efficiency through advanced natural language understanding and generation capabilities. Implemented Retrieval Augmented Generation (RAG) models to optimize the chatbot’s response accuracy, employing fine-tuning techniques that ensured high-quality interactions while adhering to responsible AI practices, including bias mitigation and fairness. Designed and managed ETL processes on AWS using Python to gather, preprocess, and integrate structured and unstructured data from various sources, including customer feedback platforms, ensuring data consistency and quality for model training and analytics. Conducted quantitative analysis using statistical models and machine learning algorithms, such as Random Forest and XGBoost, to assess customer sentiment and enhance the chatbot's ability to understand user intent through effective Generative AI applications. Collaborated with the Model Risk Management team and data governance stakeholders to create comprehensive documentation for the chatbot model, ensuring compliance with industry standards and facilitating effective deployment and monitoring of LLM performance."
      }
    ],
    "projects": [],
    "certifications": []
  },
  "Data_Scientist_4 External.docx": {
    "name": null,
    "emails": [],
    "phone_numbers": [],
    "education": [
      "Ma Learning",
      "ms Advance cyber techniques and operations",
      "Ba Learning",
      "ma XG Boost and Time series algorithms like ARIMA",
      "Old Dominion University"
    ],
    "skills": [
      "Amazon Redshift (AWS-S3)",
      "Apache Spark",
      "Azure data factory",
      "Bigdata\tHadoop",
      "C++",
      "DL",
      "Data Libraries",
      "Data Science",
      "Data bricks",
      "Data frame",
      "Data verse",
      "Database",
      "Docker",
      "ERP",
      "ETL tools\tSQL",
      "Erwin",
      "Excel",
      "Fetch XML Builder",
      "Frameworks\tNumpy",
      "GitHub",
      "HTML",
      "Hive",
      "Informatica",
      "JSON",
      "Jupiter Note",
      "Keras",
      "Kubernetes",
      "Level up for Dynamics 365 CRM",
      "Linux",
      "Logistic Regression",
      "Lucid Chart",
      "MATLAB",
      "ML",
      "MLlib",
      "MS Access",
      "Microsoft Azure ML",
      "Microsoft Word",
      "MongoDB",
      "Naïve Bayes",
      "Nearest neighbour",
      "Networking tools\tWireshark",
      "OData",
      "Operating Systems\tWindows",
      "Pandas",
      "Power Apps",
      "Power BI",
      "Power Point",
      "Power platform",
      "Programming Languages \t Python",
      "Rest API",
      "SAS",
      "SQL",
      "SSIS",
      "SSRS",
      "SVM",
      "Scikit-Learn",
      "Seaborn",
      "SharePoint",
      "Shell Scripting",
      "Snowflake",
      "Tableau",
      "Tensor Flow",
      "Terra data",
      "VMware",
      "Visual Studio",
      "XML",
      "XRM tool kit"
    ],
    "work_experiences": [
      {
        "job_title": "Unknown Title",
        "company": "Data Scientist",
        "duration_months": 20,
        "from": "Jan 2023",
        "to": "Sep 2024",
        "is_current": false,
        "work_description": "Responsibilities: Impacted Business Performance by reducing the SLA from 24 hours to 4 hours in production with 95% data accuracy. Automated the RAM documents creation process using Data frame in production data. Performed data integrity checks, data cleansing and exploratory analysis using Python libraries like Pandas, Matplotlib etc. Lead the Provider selection logic measures in UAT environment. Reviewed the work intake logs in PDASQ- Provider Data Automation and System Quality using Facets production, audit and Edinet databases. Created RAM (Roaster Automation Management) documents for Non participating practitioners from the external claims (XC). Generated matching provider ID reports for 3point Medicare, Medicaid, Medical supplement, 2point and Retiree claims with various market networks. Included /Removed the provider data based on business requirement from the Claims intake repair tool /PMRT. Build, validate, and deploy advanced statistical, machine learning, and deep learning models tailored for business problems, using Machine Learning algorithms such as Logistic Regression, Decision Tree, Naive Bayse, Random Forest, XG Boost and Time series algorithms like ARIMA Analyses patient records and medical literature to assist in diagnosis, treatment planning, and research using Deep Learning algorithms. Experience in building & configuring hyperparameter optimization pipelines with Azure and ML Flow. Data Warehouse Implementation: Led the end-to-end implementation of a data warehouse solution, enhancing data retrieval time by 40% and supporting business intelligence initiatives. Worked on Gen AI development tool called Spark for internal Associates access using NLP. Developed polished visualizations with Plotly and Dash to analyse and interpret the performance of the model. Reviewed and analysed facets claims history Tableau reports. Environment: SQL (ODW, Provrecon, Edinet1P), Facets (FAC1P, FAC1R, SPS), JIRA, Service Now, PEGA, SPS case worker, Macess, Python, Tableau Server, Pandas, Data frame"
      },
      {
        "job_title": "Unknown Title",
        "company": "Client: Competitive Innovations LLC",
        "duration_months": 41,
        "from": "Mar 2019",
        "to": "Aug 2022",
        "is_current": false,
        "work_description": "Client: Competitive Innovations LLC Responsibilities: Collaborated with data stewards and clients to test, clean, and standardize data to meet business requirements Worked on model driven apps & Power Automate(workflows) Evaluated patterns and meaningful insights from data through qualitative and quantitative analysis Created Calculated columns, measures, paginated reports and scheduled refreshes in Power BI Created user roles and groups to the end user and provided row-level security Created Employees Portal, Payroll Tracker App, HR, Timesheet report, Security clearance App, and foreign travel App. Integrated third-party application using REST API and power automate Worked on Measuring Investment Risks using Python for financial data set. Calculated Security risk, co variance, correlation, Portfolio risk and Diversifiable Risk using Python Developed python code for calculating and comparing rates of return using Numpy Worked on Capital Asset Pricing Model and regression analysis Created ad hoc Power BI reports for the business needs Environment: Microsoft Dynamics 365 CRM/ERP, AZURE data factory, Azure DevOps, API, Power APPS, Power BI, Microsoft Excel, SQL, Python, Data verse,Numpy"
      },
      {
        "job_title": "Unknown Title",
        "company": "Data Scientist",
        "duration_months": 32,
        "from": "Jul 2016",
        "to": "Mar 2019",
        "is_current": false,
        "work_description": "Responsibilities: Created ML model for Health Insurance prediction to facilitate customers with better health plans as per their needs and help organization to avoid unforeseen losses and liabilities. Analysed 10 GB health care data using data bricks, identified improved opportunities PPO blue central networks, implemented logic changes in electronic medical record system. Developed dashboards that indicate KPIs related to Claim inventory, Cleared claims, LOB, Pended Claims, External claims, daily, weekly and monthly claim insights Handled Front end logic data to import to term/activate networks, Atypical claims. Worked on QA and Production claims deployment data validations Configure and maintain data in the database servers and processes, including monitoring of system health and performance, to ensure high levels of performance, availability, and security. Experience in designing creative dashboard visualizations that captivate their respective business audience. Generated Scheduled visualization/Subscription reports to indicate the metrics. Implemented delegated table logic to select the providers based on POS, Procedure code and Speciality. Created the KPI to show the difference between actual and forecast data Implemented big data processing using Azure Databricks, enabling distributed computing for large-scale data analysis. Provided KT sessions and clarifications to the team Environment: SQL, Azure Data bricks, Excel, Power BI"
      },
      {
        "job_title": "Graduate Intern",
        "company": "Logfuze Technologies Pvt limited",
        "duration_months": 5,
        "from": "Dec 2014",
        "to": "May 2015",
        "is_current": false,
        "work_description": "Logfuze Technologies Pvt limited, India. https://logfuze.com/ Responsibilities: Processed Python, and SQL in data analytics to collect, understand, transform, cleanse, store, and share users' data, leading to increased efficiency in all processes Reported data and visualization efforts on the EDW to senior developers Experience in Tableau Server end user access privileges Worked on calculated fields, parameter controls, hierarchy, groups, sets, dashboard actions, filters, annotations Experience and familiarity with publishing, scheduling, refreshing reports to Tableau Server Built and published interactive dashboards on Tableau Server Environment:  Microsoft Excel, Python, Tableau Desktop (Version 8.2), Tableau Prep, Tableau Online, Tableau Server, SQL,"
      },
      {
        "job_title": "System Analyst",
        "company": "Mani India Technologies Private Ltd",
        "duration_months": 22,
        "from": "Jun 2007",
        "to": "Apr 2009",
        "is_current": false,
        "work_description": "Mani India Technologies Private Ltd, India. https://maniindiatech.com/ Responsibilities: Experience in logical/physical data modelling using Erwin Extensive data cleansing using pivot tables, formulas (V-Lookup and others) Experienced in Application Flow format designing and programming Worked on documenting user manual, flow diagram, functional diagram, and Process flow Extract transforms and load source data into respective target tables to build required data mart Used report builder to do Ad-hoc reporting and Performed unit testing using test cases. Environment: SQL, SSIS, SSRS, SSAS, Excel, C++, Linux, Shell scripting"
      },
      {
        "job_title": "Unknown Title",
        "company": "Lab Instructor",
        "duration_months": 11,
        "from": "Jun 2012",
        "to": "May 2013",
        "is_current": false,
        "work_description": "Sree Sastha Institute of Engineering and Technology, https://sasthainstitutions.in/"
      },
      {
        "job_title": "Unknown Title",
        "company": "Unknown Company",
        "duration_months": 12,
        "from": "Jun 2009",
        "to": "Jun 2010",
        "is_current": false,
        "work_description": "Prince Sri Venkateswara Padmavathy Engineering College, https://www.psvpec.in/"
      }
    ],
    "projects": [],
    "certifications": []
  }
}