{
   "pdf": {},
   "docx": {
      "C:/Flexon_Resume_Parser/Parser_Build-Arnav/Test Resumes/Sample Resumes/AI Engineer.docx": {
         "name": "Bert",
         "emails": [
            "email@mail.com"
         ],
         "phone_numbers": [
            "(123) 456-7890"
         ],
         "urls": [],
         "education": [
            "University of Texas at Dallas - Master of Science in Business Analytics;"
         ],
         "skills": [
            "algorithms like Random Forest",
            "Environment: AWS Kinesis",
            "Python",
            "Data Pipeline",
            "and LangChain",
            "flight route popularity",
            "historical pricing trends",
            "AWS Lambda",
            "system performance",
            "and data anomalies",
            "Google Dialogflow",
            "Google BARD",
            "AWS CloudWatch",
            "and LSTM",
            "Southwest Airlines",
            "Dallas",
            "on Amazon SageMaker",
            "relevancy ranking",
            "Prophet",
            "AWS Glue",
            "Power BI",
            "AWS S3",
            "facilitating automated",
            "and user query resolution",
            "Machine Learning",
            "genism)",
            "LLM (Llama",
            "Apache Kafka",
            "and many more",
            "Snowflake",
            "Built scalable",
            "PySpark",
            "Bert"
         ],
         "work_experiences": [
            {
               "job_title": "AI/ML Engineer",
               "company": "NerdWallet",
               "duration_months": 22.1,
               "from": "Nov 2023",
               "to": "Present",
               "is_current": true,
               "work_description": "NerdWallet, Remote. Designed advanced Generative AI (GEN AI) powered applications using Large Language Models (LLMs), NLP, Google Dialogflow, and Retrieval-Augmented Generation (RAG), improving customer interaction experiences by providing contextually relevant and accurate responses. Engineered end-to-end machine learning pipelines, integrating GPT (LLM) for natural language processing (NLP), natural language understanding (NLU), relevancy, and ranking in document retrieval, enhancing automated support and content generation workflows. Led the PoC of a scalable GPT (LLM)-based chatbot using Decoder Transformer architecture, Google Dialogflow, and LangChain, facilitating automated, human-like responses for customer support. Built and maintained real-time data pipelines using Apache Kafka and Google Cloud Pub/Sub, enabling low-latency streaming for machine learning models and improving response times for customer queries. Implemented document retrieval systems with NLP-based query expansion and semantic search, improving the relevancy and ranking of search results for customer support applications powered by Google Dialogflow for enhanced user interaction. Applied NLP techniques to analyze customer feedback and reviews, extracting key insights using sentiment analysis and topic modeling, helping product teams prioritize feature enhancements. Implemented Google AI Platform for training and deploying machine learning models, ensuring efficient handling of large datasets and automated scaling for prediction services, reducing model deployment time and enhancing relevancy in predictions. Applied RAG techniques, combining traditional machine learning and information retrieval systems, improving the ranking, relevancy, and accuracy of customer query resolution through Google Dialogflow. Collaborated with data science teams to integrate GPT-based and NLP models for content summarization, relevancy ranking, and user query resolution, significantly improving the product knowledgebase. Developed CI/CD pipelines using GitLab and Google Cloud Build to streamline the deployment of machine learning models, ensuring rapid iteration, versioning, and relevancy improvements across production environments. Enhanced model performance with distributed systems using Apache Spark on Google Cloud Dataproc, accelerating data processing times and enabling real-time model inference with accurate ranking. Collaborated with DevOps and SRE teams to ensure high availability and monitoring of AI systems, optimizing infrastructure using Google Kubernetes Engine (GKE) for orchestration and deployment. Integrated Google Identity Platform for secure, scalable user authentication in AI-driven customer support platforms using Google Dialogflow, enhancing data privacy and secure access to sensitive data. Environment: LLMs, RAG, Google Dialogflow, Hugging Face, LangChain, Python, Google AI Platform, Apache Kafka, Google Cloud Pub/Sub, Google Cloud Dataproc, Spark, GitLab, Google Kubernetes Engine, Docker, Google Identity Platform, ELK Stack, CI/CD"
            },
            {
               "job_title": "ML Engineer",
               "company": "Honda Motor Company",
               "duration_months": 46,
               "from": "Jan 2020",
               "to": "Nov 2023",
               "is_current": false,
               "work_description": "Honda Motor Company, Remote. Architected and implemented a scalable real-time data pipeline using AWS Kinesis, Lambda, and S3 to process and onboard telemetry data for real-time analytics, ensuring relevancy of streaming data analysis. Developed data onboarding and processing workflows leveraging Managed Streaming for Apache Kafka (MSK) and PySpark to stream, enrich, and transform vehicle telemetry data, integrating NLP algorithms for automated text extraction and ranking of insights based on data importance. Architected the cloud infrastructure using AWS CloudFormation (IAC) to automate the provisioning of resources like S3, EC2, RDS, Redshift, etc., optimizing for high relevancy in data flow and processing. Integrated Snowflake as the central data warehouse, optimizing storage and query performance through partitioned tables, SQL-based transformations, and NLP-driven data categorization, ensuring faster retrieval and ranking of critical data. Collaborated with data scientists to design and deploy machine learning models using Python, PySpark, and SageMaker, applying ranking algorithms to prioritize vehicle component failure predictions and trigger proactive maintenance based on high-relevancy alerts. Applied AWS Glue Data Catalog to organize and manage metadata, utilizing NLP to classify and rank data for efficient access and analysis, ensuring relevant data is prioritized in workflows. Developed and maintained large-scale data pipelines using Apache Spark, MapReduce, and PySpark, with MSK handling big data processing, ensuring that high-relevancy data is ranked, for faster ETL workflows. Implemented supervised learning techniques, including Support Vector Machines (SVM) and Decision Trees, on Amazon SageMaker, with NLP techniques applied to classify and rank incoming data. Leveraged Scikit-learn, TensorFlow, and PySpark for training machine learning models on Amazon SageMaker, including NLP models for real-time text extraction and ranking based on relevancy to vehicle performance and user behavior. Developed and deployed MLOps pipelines in production utilizing SageMaker, SageMaker-Data Wrangler, Elastic Kubernetes Service (EKS), and PySpark, incorporating NLP-driven ranking systems to enhance data quality and relevance for real-time analysis. Managed the end-to-end data pipeline lifecycle, from architecture design to deployment and optimization, ensuring high availability and reliability of PySpark-based pipelines, with ranking algorithms to prioritize data streams based on their relevancy for downstream processes. Developed custom ETL processes using AWS Glue, PySpark, and SQL to transform raw data into actionable insights, incorporating NLP and ranking models to improve downstream. Environment: AWS Kinesis, AWS Lambda, AWS S3, Apache Kafka, Snowflake, Python, AWS Glue, AWS CloudWatch, Power BI, Machine Learning, Data Pipeline, Terraform (Infrastructure as Code - IAC)"
            },
            {
               "job_title": "Data Scientist",
               "company": "Southwest Airlines",
               "duration_months": 32,
               "from": "Mar 2017",
               "to": "Nov 2019",
               "is_current": false,
               "work_description": "Southwest Airlines, Dallas, TX"
            }
         ],
         "total_experience_years": 8.34
      },
      "C:/Flexon_Resume_Parser/Parser_Build-Arnav/Test Resumes/Sample Resumes/AI ML Engineer.docx": {
         "name": "ANN",
         "emails": [
            "email@mail.com"
         ],
         "phone_numbers": [
            "(123) 456-7890"
         ],
         "urls": [],
         "education": [
            "Masters in Data Science – New Jersey Institute of Technology, Newark, NJ"
         ],
         "skills": [
            "and GPT models",
            "analysis",
            "Citibank",
            "scikit-learn",
            "versioning",
            "querying",
            "and collaboration skills",
            "Proficient in Python",
            "Agentic",
            "including Transformers",
            "LangChain",
            "scalability",
            "Newark",
            "evaluating",
            "optimizing model deployment",
            "Excellent problem-solving",
            "NumPy",
            "specializing in ML",
            "Mooresville",
            "Lowe's",
            "with a focus on Azure",
            "Expertise in developing",
            "Expertise in Data Science",
            "KNN",
            "Chicago",
            "Wells Fargo",
            "communication",
            "New York",
            "BERT",
            "ANN"
         ],
         "work_experiences": [
            {
               "job_title": "Data Scientist",
               "company": "Wells Fargo",
               "duration_months": 40.1,
               "from": "May 2022",
               "to": "Present",
               "is_current": true,
               "work_description": "Developed and deployed a generative AI-driven financial forecasting system using Python, SQL, and Azure ML, leveraging time series models such as ARIMA and Prophet for accurate market trend predictions. Implemented customer segmentation models using Python with KNN and Random Forest algorithms to categorize clients based on transaction behaviors, improving personalized marketing strategies. Built and fine-tuned Llama 2-based generative AI chatbots on Azure OpenAI using Python and ANN architectures to handle real-time customer queries and financial advice, increasing customer engagement and support efficiency. Utilized mlFlow to manage the end-to-end lifecycle of machine learning models, including tracking, versioning, and deploying predictive models for financial forecasting with KNN and ANN architectures. Integrated vector databases like Pinecone to store and retrieve high-dimensional embeddings, enabling fast, accurate responses from the generative AI chatbot and improving its adaptability to complex queries. Applied ANN and Transformer architectures for natural language processing tasks, enhancing chatbot accuracy and response time. Optimized time series models using Python for real-time forecasting of financial product demand, implementing mlFlow for model version control, and using advanced seasonal decomposition techniques. Monitored and evaluated model performance with mlFlow and Azure Machine Learning tools, continuously optimizing models with KNN and ANN based on performance metrics and business KPIs, ensuring reliable forecasting. Collaborated with cross-functional teams to integrate Python machine learning models into a scalable, cloud-based infrastructure on Azure, deploying ANN and KNN algorithms for diverse AI-powered services. Developed automated drift detection systems using Azure ML, ensuring continuous model adaptation and retraining to maintain high prediction accuracy, with mlFlow tracking for drift handling."
            },
            {
               "job_title": "Data Scientist",
               "company": "Citibank",
               "duration_months": 33,
               "from": "Jul 2019",
               "to": "Apr 2022",
               "is_current": false,
               "work_description": "Developed machine learning models using Python, employing popular libraries such as scikit-learn and pandas to automate the analysis of financial marketing content, enhancing customer engagement and improving campaign efficiency. Utilized SQL for efficient data extraction and preprocessing, ensuring the integrity and scalability of large financial datasets for training machine learning models in Python. Implemented ML models including Random Forest and KNN for customer segmentation, and leveraged Time Series Forecasting models in Python to predict key financial metrics such as demand fluctuations, customer behavior trends, and campaign performance. Designed and deployed Neural Network Architectures (ANN) and Transformers, incorporating BERT to analyze and generate financial content tailored to customer needs, enhancing personalized marketing strategies in financial services. Worked with Open Source Foundational Models like LLama2, customizing them for the financial domain, and integrated GPT models for generating financial advisory content and chatbot interactions that provide real-time, context-aware assistance to customers. Managed machine learning model life cycles using mlFlow, enabling seamless model tracking, versioning, and performance evaluation for KNN and ANN models, ensuring continuous improvements in model accuracy and deployment efficiency. Utilized Azure Machine Learning and Azure OpenAI for cloud-based ML services, optimizing model deployment, scalability, and integration with existing business systems in the cloud. Integrated Vector Databases to store and retrieve financial data in a high-dimensional space, enhancing the effectiveness of LLM/GEN-AI tools and frameworks like LangChain and Agentic for advanced query handling and content generation. Implemented performance evaluation mechanisms in mlFlow to assess model accuracy, scalability, and drift handling in both ML/DL systems and KNN/ANN models, ensuring deployed solutions remained responsive to evolving customer needs and financial trends."
            },
            {
               "job_title": "Data Analyst",
               "company": "Lowe's",
               "duration_months": 19,
               "from": "Dec 2017",
               "to": "Jul 2019",
               "is_current": false,
               "work_description": "Developed and implemented machine learning models using Python to enhance retail inventory management with a focus on demand forecasting and stock optimization, ensuring improved product availability and reducing stockouts. Leveraged advanced techniques to predict sales trends and manage inventory dynamically. Engineered automated ETL pipelines using Azure Data Factory, enabling real-time extraction, transformation, and loading (ETL) of sales and inventory data from multiple sources. This ensured data integrity and readiness for ML model training and evaluation. Applied advanced statistical techniques and machine learning algorithms, including Time Series Analysis, Random Forest, KNN, and Regression Models to enhance forecasting accuracy for inventory replenishment, improving operational efficiency and ensuring timely stock replenishment in retail settings. Developed interactive dashboards with Power BI to visualize critical KPIs related to inventory turnover, demand trends, and sales projections, supporting data-driven decision-making and optimized supply chain management in retail operations. Integrated machine learning models with retail systems using Azure Functions for serverless execution, providing real-time access to predictive insights and optimizing inventory management and supply chain operations, enhancing both operational efficiency and decision-making. Utilized Azure Machine Learning (Azure ML) for scalable, automated model training and deployment, managing the end-to-end machine learning lifecycle, from model development to real-time deployment in production environments. Ensured data security through Azure Active Directory (AAD) and Azure Key Vault, protecting sensitive retail data and ensuring compliance with industry regulations, thereby safeguarding both customer and business data. Monitored and fine-tuned model performance using Azure Monitor and Azure Application Insights, ensuring optimal resource utilization and quickly adapting to changing sales patterns and business needs, improving prediction accuracy and reliability over time. Leveraged Azure SQL Database for data storage, enabling high availability, scalability, and secure management of transactional and historical retail data, allowing efficient querying and reporting to optimize inventory. Collaborated with cross-functional teams, including data analysts, operations staff, and supply chain managers, ensuring alignment between machine learning initiatives and business goals. This collaboration optimized inventory levels and enhanced customer experience through improved product availability. Documented machine learning workflows and system architecture, providing comprehensive training and support for team members, ensuring seamless deployment and operational integration of machine learning solutions into retail operations."
            }
         ],
         "total_experience_years": 7.67
      },
      "C:/Flexon_Resume_Parser/Parser_Build-Arnav/Test Resumes/Sample Resumes/Cloud Engineer.docx": {
         "name": "Docker",
         "emails": [],
         "phone_numbers": [],
         "urls": [],
         "education": [
            "University of Texas at Dallas - Master of Science in Business Analytics;"
         ],
         "skills": [
            "CloudFormation",
            "AWS Lambda",
            "AWS Kinesis",
            "Glue",
            "Terraform (IAC)",
            "and Kubernetes",
            "Dallas",
            "IND",
            "Amazon Lex",
            "streamlining model training",
            "AWS Redshift",
            "testing",
            "enrich telemetry data",
            "transformation",
            "Docker",
            "and AWS SQS",
            "Lambda",
            "Power BI",
            "Git",
            "RDS",
            "CI/CD",
            "Honda Motor Company",
            "Environment: Python",
            "Amazon RDS",
            "Kubernetes (EKS)",
            "enabling scalable",
            "CloudWatch",
            "NerdWallet",
            "security",
            "SQL",
            "and storage optimization",
            "and SQS",
            "failure points",
            "CloudTrail",
            "Bengaluru",
            "Amazon Kinesis",
            "SNS",
            "TensorFlow",
            "and predictive maintenance",
            "achieving high availability",
            "analyze",
            "operational data",
            "PostgreSQL",
            "ETL",
            "PySpark",
            "Southwest Airlines",
            "and PySpark to stream",
            "Environment: AWS Kinesis",
            "ensuring reliability",
            "including data ingestion",
            "Redshift",
            "AWS CloudWatch",
            "and Datadog",
            "Environment: AWS S3",
            "and AWS CodePipeline",
            "AWS Glue",
            "AWS SageMaker",
            "CloudFormation (IaC)",
            "EC2",
            "and Amazon SQS",
            "detect data drift",
            "Leveraged Scikit-learn",
            "access control",
            "Python",
            "Data Pipeline",
            "Amazon SNS",
            "scalability",
            "AWS EKS",
            "RDS)",
            "Kubernetes",
            "Bengalore",
            "Torrence",
            "Jenkins",
            "Environment: AWS Lambda",
            "GitHub",
            "Amazon Cognito",
            "Apache Kafka",
            "Snowflake",
            "and external sources",
            "enhancing scalability",
            "SQS"
         ],
         "work_experiences": [
            {
               "job_title": "Sr. Data Engineer",
               "company": "Honda Motor Company",
               "duration_months": 68.1,
               "from": "Jan 2020",
               "to": "Present",
               "is_current": true,
               "work_description": "Honda Motor Company, Torrence, CA. Architected and implemented a scalable real-time data pipeline using AWS Kinesis, Lambda, S3, and SQS to process and onboard telemetry data, ensuring efficient real-time analytics. Developed data onboarding and processing workflows leveraging Managed Streaming for Apache Kafka (MSK), SNS, and PySpark to stream, enrich telemetry data, achieving real-time data availability. Architected cloud infrastructure on AWS using AWS CloudFormation to automate provisioning of resources like S3, EC2, RDS, Redshift, SNS, and SQS, streamlining resource management. Integrated Snowflake as the central data warehouse, optimizing storage and query performance through partitioned tables and SQL-based transformations for advanced analytics. Collaborated with data scientists to design and deploy machine learning models in Kubernetes and SageMaker, enabling scalable, containerized deployments and model predictions oncomponent failures. Used AWS Glue Data Catalog and SageMaker-Data Wrangler to manage metadata, process large datasets, and perform data transformations, facilitating efficient data monitoring and analysis. Developed and managed large-scale data pipelines using MapReduce, PySpark, and MSK for big data processing in ETL workflows, incorporating SQS to manage message queuing across processes. Automated deployment and management of data pipeline components using AWS CloudFormation (IaC), Kubernetes, and GitHub CI/CD ensuring environment consistency and scaling capabilities. Leveraged Scikit-learn, TensorFlow, and PySpark to train machine learning models on SageMaker, facilitating scalable model deployment and efficient processing of telemetry and transaction data. Designed and delivered Power BI dashboards for insights into vehicle performance, failure points, and predictive maintenance, supporting data-driven decisions. Collaborated with DevOps to establish CI/CD pipelines using GitHub, Jenkins, Kubernetes, and AWS CodePipeline, supporting efficient deployment and version control for data infrastructure. Deployed and maintained MLOps pipelines in production with SageMaker-Data Wrangler and Elastic Kubernetes Service (EKS), enhancing data quality and enabling model retraining. Managed end-to-end data pipeline lifecycle from design to deployment, ensuring reliability, scalability, and high availability for PySpark-based pipelines. Created custom ETL processes with AWS Glue, PySpark, SQS, and SQL to transform raw data into actionable insights, optimizing data for advanced analytics and machine learning. Environment: AWS Kinesis, AWS Lambda, S3, Glue, CloudWatch, SQS, SNS, RDS, Redshift, Kubernetes, Apache Kafka, Snowflake, Python, PySpark, Power BI, GitHub, CI/CD, CloudFormation (IaC)"
            },
            {
               "job_title": "Data Science Engineer",
               "company": "Southwest Airlines",
               "duration_months": 22,
               "from": "Aug 2016",
               "to": "Jun 2018",
               "is_current": false,
               "work_description": "Southwest Airlines, Dallas, TX Designed and implemented scalable data pipelines using AWS Glue and PySpark, automating ETL processes for handling large volumes of structured and unstructured data, integrating them into Snowflake. Developed Infrastructure-as-Code (IaC) using CloudFormation to provision and manage AWS resources, ensuring consistency across environments, enhancing scalability, and reducing configuration errors. Built real-time data streaming applications with AWS Kinesis, MSK (Managed Streaming for Apache Kafka), and AWS SQS, processing millions of events per second for live analytics and message queuing. Led the migration of legacy on-premises data pipelines to the AWS Cloud using CloudFormation (IaC), achieving high availability, security, and scalability across multiple environments. Leveraged Kubernetes for managing distributed containers, ensuring efficient resource management and high availability, and utilized SNS and SQS for decoupled communication within architecture. Developed and managed data workflows with PySpark, leveraging Git for version control and GitHub CI/CD pipelines for streamlined operations across development, testing, and production environments. Collaborated with DevOps to implement CI/CD pipelines using GitHub, Jenkins, and AWS CodePipeline, ensuring seamless deployment and version control for infrastructure and data pipelines. Implemented real-time monitoring and anomaly detection with AWS CloudWatch, Amazon SNS, and Amazon SQS, creating custom dashboards and setting up alerts for proactive system health monitoring. Deployed containerized applications using Docker and managed orchestration through Kubernetes (EKS), facilitating efficient scaling and resource allocation for data pipelines, enhancing fault tolerance. Built and optimized data lakes on Amazon S3 and AWS Redshift for storing and analyzing terabytes of historical and real-time transactional data, improving data accessibility and scalability. Enhanced data pipeline resilience and scalability through PySpark-based batch and real-time processing workflows, optimizing performance for distributed cloud infrastructure. Streamlined batch and real-time data processing using PySpark to optimize distributed data pipelines in the cloud, enhancing performance and reducing latency. Environment: Python, SQL, PostgreSQL, Snowflake, AWS Glue, CloudFormation, AWS Kinesis, Docker, Kubernetes (EKS), AWS CloudWatch, SNS, SQS, S3, Redshift, Git, GitHub, CI/CD, Jenkins, PySpark."
            },
            {
               "job_title": "AWS Data Engineer",
               "company": "Berkshire Hathaway Homestate Companies (BHHC)",
               "duration_months": 30,
               "from": "Jun 2018",
               "to": "Dec 2020",
               "is_current": false,
               "work_description": "Berkshire Hathaway Homestate Companies (BHHC), Bengalore, IND Architected the cloud infrastructure on AWS using AWS CloudFormation (IAC) to automate the provisioning of resources like S3, EC2, RDS, Redshift, etc. Architected the end-to-end data flow from AWS S3 to Snowflake, ensuring efficient data onboarding, transformation, and loading processes while minimizing latency. Developed custom ETL processes using AWS Glue and SQL to transform raw data into actionable insights, optimizing data for downstream analytics. Collaborated with DevOps to implement CI/CD pipelines using GitHub, Jenkins, and AWS CodePipeline, ensuring seamless deployment and version control for infrastructure and data pipelines. Implemented robust error handling mechanisms, including retry logic and alerting, to ensure data pipeline reliability and quick resolution of data processing failures. Automated the deployment and management of data pipeline components using AWS CloudFormation (IAC), reducing setup time and ensuring consistency across cloud platforms. Integrated AWS Lambda functions to trigger real-time data processing workflows based on specific events, improving data timeliness for business-critical operations. Conducted performance tuning of SQL queries and Snowflake data warehouse configurations to enhance query speed and reduce processing costs. Optimized data partitioning and storage strategies in Snowflake, ensuring the pipeline could efficiently handle varying data volumes without performance degradation. Environment: AWS Lambda, AWS Glue, Redshift, Snowflake, SQL, CloudWatch, ETL, Data Pipeline, CloudTrail, CloudFormation, Terraform (IAC)"
            },
            {
               "job_title": "Data Engineer",
               "company": "NerdWallet",
               "duration_months": 23,
               "from": "Aug 2014",
               "to": "Jul 2016",
               "is_current": false,
               "work_description": "NerdWallet, Bengaluru, IND Designed and optimized end-to-end ETL pipelines using AWS services (S3, Glue, Redshift, RDS), ensuring automated T-1 data extraction, transformation, and loading of data. Engineered and implemented an AI-powered chat/voice bot using Amazon Lex to replace the legacy IVR system, automating customer support processes and leading to a reduction in manual intervention. Conducted extensive data analysis on customer support interactions, leveraging machine learning to extract actionable insights and inform product update strategies. Enhanced ETL pipelines to extract data form Salesforce email campaign data, operational data, and external sources, achieving increase in performance efficiency. Developed and deployed machine learning models in production utilizing AWS SageMaker and EKS to enhance data quality, achieving an improvement in overall data utilization. Developed live dashboards for real-time issue tracking using Power BI, enabling the software development and product teams to monitor, analyze, and address the most frequent customer complaints. Collaborated with marketing and customer support teams to implement targeted retention strategies based on churn predictions, directly contributing to improvement in customer satisfaction. Established more secure customer profiles using Amazon Cognito, further improving customer privacy. Environment: AWS S3, AWS Glue, AWS Redshift, Amazon RDS, Amazon Lex, Amazon Kinesis, AWS SageMaker, AWS EKS, Python, Power BI, ETL, Amazon Cognito University of Texas at Dallas - Master of Science in Business Analytics; Graduate Certificate: Specialization in Applied Machine Learning."
            }
         ],
         "total_experience_years": 11.92
      },
      "C:/Flexon_Resume_Parser/Parser_Build-Arnav/Test Resumes/Sample Resumes/Data Engineer.docx": {
         "name": "Docker",
         "emails": [
            "email@mail.com"
         ],
         "phone_numbers": [
            "(123) 456-7890"
         ],
         "urls": [],
         "education": [
            "University of Texas at Dallas - Master of Science in Business Analytics; Specialization in Applied Machine Learning."
         ],
         "skills": [
            "Environment: AWS Kinesis",
            "CloudWatch",
            "access control",
            "Python",
            "including data ingestion",
            "Data Pipeline",
            "Redshift",
            "NerdWallet",
            "AWS Lambda",
            "operational data",
            "Datadog",
            "and feature engineering",
            "Glue",
            "AWS CloudWatch",
            "and Kubernetes",
            "potential failure points",
            "and Datadog",
            "San Francisco",
            "ensuring data encryption",
            "SQL",
            "and storage optimization",
            "normalization",
            "reducing response times",
            "AWS EKS",
            "RDS)",
            "Environment: AWS S3",
            "CloudTrail",
            "Amazon Lex",
            "streamlining model training",
            "AWS Redshift",
            "testing",
            "Amazon Kinesis",
            "transformation",
            "Docker",
            "Lambda",
            "AWS Glue",
            "Power BI",
            "Environment: AWS Lambda",
            "AWS S3",
            "Omaha",
            "RDS",
            "AWS SageMaker",
            "Machine Learning",
            "Amazon Cognito",
            "Apache Kafka",
            "analyze",
            "EC2",
            "Amazon RDS",
            "Snowflake",
            "and external sources",
            "including data cleaning",
            "ETL",
            "detect data drift"
         ],
         "work_experiences": [
            {
               "job_title": "Sr. Data Engineer",
               "company": "Honda Motor Company",
               "duration_months": 39.1,
               "from": "Jun 2022",
               "to": "Present",
               "is_current": true,
               "work_description": "Honda Motor Company, Remote. Architected and implemented a scalable real-time data pipeline using AWS Kinesis, Lambda, and S3 to process telemetry data. Developed data ingestion and processing workflows leveraging Apache Kafka to stream and enrich vehicle telemetry data, ensuring real-time data availability. Integrated Snowflake as the central data warehouse, optimizing storage and query performance through partitioned tables for advanced analytics. Collaborated with data scientists to design and deploy machine learning models in Python that predict vehicle component failures, triggering proactive maintenance alerts. Established monitoring and log systems using AWS CloudWatch and Datadog, optimizing data flow to handle peak loads with consistent throughput and minimal latency. Worked with cross-functional teams, including software engineers and product managers, to define technical requirements and ensure seamless pipeline integration. Implemented data governance practices, ensuring data encryption, access control, and compliance with data privacy regulations across all telemetry data processes. Automated data pipeline workflows using AWS Glue and Lambda, reducing manual intervention and improving data freshness for real-time analytics. Set up a monitoring framework using Datadog to track data pipeline performance and flow, allowing for proactive issue detection and logging. Designed and delivered dashboards and reports in Power BI, providing insights into vehicle performance, potential failure points, and predictive maintenance effectiveness. Improved predictive maintenance accuracy, reducing unexpected vehicle breakdowns and enhancing customer satisfaction. Reduced overall maintenance costs through proactive identification and resolution of vehicle issues before they become critical. Achieved sub-second data processing latency, ensuring timely and accurate data for predictive analytics and decision-making. Managed the end-to-end data pipeline lifecycle, from initial architecture to deployment and ongoing optimization, ensuring high availability and reliability. Executed exploratory data analysis to uncover patterns and relationships in customer behavior, which guided feature selection and informed the development of more effective predictive models. Visualized prediction outcomes using Power BI, creating interactive dashboards and comprehensive reports to effectively communicate insights and predictions to key stakeholders. Delivered knowledge transfer sessions and detailed documentation to enable smooth handover of the solution to the operations and maintenance teams. Environment: AWS Kinesis, AWS Lambda, AWS S3, Apache Kafka, Snowflake, Python, AWS Glue, AWS CloudWatch, Power BI, Machine Learning, Data Pipeline, Datadog"
            },
            {
               "job_title": "AWS Data Engineer",
               "company": "Berkshire Hathaway Homestate Companies (BHHC)",
               "duration_months": 29,
               "from": "Jan 2020",
               "to": "Jun 2022",
               "is_current": false,
               "work_description": "Berkshire Hathaway Homestate Companies (BHHC), Omaha, NE Architected the cloud infrastructure on AWS using AWS CloudFormation to automate the provisioning of resources like S3, EC2, RDS, Redshift, etc. Designed and implemented scalable data pipelines that integrated AWS services (S3, RDS, Redshift, Glue) with Snowflake to enable seamless data processing and storage. Architected the end-to-end data flow from AWS S3 to Snowflake, ensuring efficient data ingestion, transformation, and loading processes while minimizing latency. Developed custom ETL processes using AWS Glue and SQL to transform raw data into actionable insights, optimizing data for downstream analytics. Performed extensive data preprocessing, including data cleaning, normalization, and feature engineering, to prepare large-scale historical data for accurate analysis and predictive modeling. Implemented robust error handling mechanisms, including retry logic and alerting, to ensure data pipeline reliability and quick resolution of data processing failures. Automated the deployment and management of data pipeline components using AWS CloudFormation, reducing setup time and ensuring consistency across environments. Integrated AWS Lambda functions to trigger real-time data processing workflows based on specific events, improving data timeliness for business-critical operations. Designed the security architecture with IAM roles and policies. Utilized AWS KMS for encryption to protect data in transit and at rest. Configured AWS Redshift as a temporary staging environment for large datasets, allowing for efficient data aggregation before final transfer to Snowflake. Conducted performance tuning of SQL queries and Snowflake data warehouse configurations to enhance query speed and reduce processing costs. Set up a monitoring framework using Datadog to track data pipeline performance and flow, allowing for proactive issue detection and logging. Optimized data partitioning and storage strategies in Snowflake, ensuring the pipeline could efficiently handle varying data volumes without performance degradation. Delivered detailed documentation and knowledge transfer sessions to ensure the smooth handover of the solution to the operations team. Achieved near-zero downtime with the implementation of robust error handling and automated recovery mechanisms. Environment: AWS Lambda, AWS Glue, Redshift, Snowflake, SQL, CloudWatch, ETL, Data Pipeline, CloudTrail, Datadog"
            },
            {
               "job_title": "Data Engineer",
               "company": "NerdWallet",
               "duration_months": 20,
               "from": "Mar 2018",
               "to": "Nov 2019",
               "is_current": false,
               "work_description": "NerdWallet, San Francisco, CA Designed and optimized end-to-end ETL pipelines using AWS services (S3, Glue, Redshift, RDS), ensuring automated T-1 data extraction, transformation, and loading of data. Engineered and implemented an AI-powered chat/voice bot using Amazon Lex to replace the legacy IVR system, automating customer support processes and leading to a 60% reduction in manual intervention. Created and managed data streaming pipelines using Amazon Kinesis, enabling the company to process and analyze customer behavior data in real time, allowing the company to respond quickly to changes in customer preferences and market trends. Conducted extensive data analysis on customer support interactions, leveraging machine learning to extract actionable insights and inform product update strategies. Enhanced ETL pipelines to extract data form Salesforce email campaign data, operational data, and external sources, achieving increase in performance efficiency. Developed and deployed machine learning models in production utilizing AWS SageMaker and EKS to enhance data quality, achieving an improvement in overall data utilization. Developed live dashboards for real-time issue tracking using Power BI, enabling the software development and product teams to monitor, analyze, and address the most frequent customer complaints. Collaborated with marketing and customer support teams to implement targeted retention strategies based on churn predictions, directly contributing to improvement in customer satisfaction. Performed extensive data preprocessing, including data cleaning, normalization, and feature engineering, to prepare large-scale historical data for accurate analysis and predictive modelling. Improved customer satisfaction by streamlining the support process, reducing response times, and providing the development team with insights to address common issues proactively. Implemented data security and privacy measures in compliance with industry regulations, ensuring data protection and governance. Established more secure customer profiles using Amazon Cognito, further improving customer privacy. Environment: AWS S3, AWS Glue, AWS Redshift, Amazon RDS, Amazon Lex, Amazon Kinesis, AWS SageMaker, AWS EKS, Python, Power BI, ETL, Amazon Cognito"
            }
         ],
         "total_experience_years": 7.34
      },
      "C:/Flexon_Resume_Parser/Parser_Build-Arnav/Test Resumes/Sample Resumes/Data Scientist AI_ML Engineer.docx": {
         "name": "Seaborn",
         "emails": [
            "email@mail.com"
         ],
         "phone_numbers": [
            "(123) 456-7890"
         ],
         "urls": [],
         "education": [],
         "skills": [
            "AUC-ROC curve",
            "deploying models",
            "Pandas",
            "data collection",
            "and partners",
            "epochs",
            "adjusted R square",
            "data wrangling",
            "tune",
            "machine learning modeling",
            "predictive algorithms",
            "and automation",
            "activation function",
            "dropouts",
            "batch size",
            "Lambda",
            "and initial weights",
            "model tunning",
            "train",
            "PCA",
            "product managers",
            "Developed system models",
            "EMR",
            "EC2",
            "confusion matrix",
            "max pooling"
         ],
         "work_experiences": [
            {
               "job_title": "Data Scientist",
               "company": "Verizon",
               "duration_months": 44.1,
               "from": "Jan 2022",
               "to": "Present",
               "is_current": true,
               "work_description": "Developed system models, predictive algorithms, and solutions for prescriptive analytics problems, applying advanced data mining techniques to large datasets to drive business insights Analyzed business problems by using appropriate statistical models such as regression analysis, causal modeling, and leveraged Pandas and NumPy for data manipulation to generate meaningful insights and support data-driven decision-making Built and deployed statistical and machine learning models to solve large-scale, customer-focused problems, using statistical methods like decision trees, clustering, SVM, and neural networks to address business challenges Conducted preliminary and exploratory data analysis (EDA) using descriptive statistics and visualization libraries like Matplotlib and Seaborn, handled anomalies such as missing data imputation and duplicate removal to ensure data quality and integrity Applied machine learning algorithms including decision trees, NLP, regression models, clustering, and neural networks, using Python’s Scikit-learn package alongside Pandas and NumPy to identify key patterns and volumes, improving business processes Performed data cleaning and feature selection using MLlib in PySpark and worked with deep learning frameworks like TensorFlow to build Artificial Neural Networks (ANNs) and Convolutional Neural Networks (CNNs) for predictive modeling, including churn rate prediction Applied causal modeling to identify cause-and-effect relationships within customer behavior, enhancing model accuracy and improving stakeholder decision-making Built predictive models using ensemble methods such as gradient boosting and neural networks (Keras) to forecast outcomes like sales amounts, enhancing business forecasting accuracy Performed feature engineering techniques including label encoding, PCA, and feature normalization using Scikit-learn, Pandas, and NumPy for model optimization and improving predictive accuracy Collaborated with data engineers and operational teams to implement ETL processes, writing and optimizing SQL queries for data extraction, transformation, and loading to fit analytical and reporting needs Leveraged python for efficient data manipulation and created robust data models to ensure the accuracy and reliability of insights and utilized python for efficient data manipulation and created robust data models to ensure the accuracy and reliability of insights Utilized Object-Oriented Programming (OOP) principles in Python to design reusable code structures, implementing classes and methods for better organization of complex data analysis workflows, enhancing maintainability and scalability of projects Collaborated with cross-functional teams to communicate insights, explain statistical concepts in clear terms, and drive data-informed decisions across the organization Developed interactive dashboards and visualizations using Tableau to communicate complex data-driven insights, ensuring stakeholders and decision-makers can access and interpret information easily Applied Tableau for building data visualizations that highlight key trends and patterns from complex datasets, improving data-driven decision-making across various business functions"
            },
            {
               "job_title": "Data Scientist",
               "company": "Responsible for clarifying business objective",
               "duration_months": 28,
               "from": "Jul 2019",
               "to": "Nov 2021",
               "is_current": false,
               "work_description": "Responsible for clarifying business objective, data collection, data wrangling, data processing, machine learning modeling, model tunning, deploying models Worked with cross-functional teams to understand the data requirements and optimize problem-solving skills to find the necessary data to support business problems Leveraged Pandas and NumPy for efficient data manipulation including handling missing values and performing data aggregations to deliver meaningful insights from the data and employed OOP concepts to strucutre and encapsulate data processing methods effectively Utilized Pandas DataFrames to merge and join datasets effectively, ensuring a comprehensive view of customer behavior and performance metrics for enhanced analysis Performed cluster analysis to access sales agent performance and gain insights by performing segmentation tasks that group agents with similar performance metrics together using Python packages Applied regression analysis to optimize call volume by examining the relationship between call volume and influencing factors to predict and refine the call volume recommendations that help to reduce the abandoned call rate Assessed the potential risk associated with different customer segments and used statistical modeling and regression analysis to model and analyze the risk score with different independent variables and conducted various statistical analysis including ANOVA and many hypotheses testing methods Visualized complex datasets and communicated insights using Tableau to build interactive dashboards for stakeholders, enabling data-driven decision-making Created Tableau reports to effectively monitor key performance indicators (KPIs) and provide ad-hoc visual analysis to explore trends and patterns Designed Tableau dashboards for executives, allowing for seamless tracking of operational performance and enabling high-level data visualization to support strategy discussions Created database schemas to support data integrity and optimize storage solutions, facilitating effective data management for analytics and modeling tasks Evaluated model performance using techniques like R square, adjusted R square, confusion matrix, AUC-ROC curve Communicate insights with third-party partners such as marketing agencies and CRM platforms to target and personalize outreach strategies Collaborate with data engineering team to extract customers data from various sources and assured data quality and data integrity and optimized data collection procedures on a weekly and monthly basis using Python Collaborated with the data scientist team and BA team to analyze on building a predictive model based on the requirements using various ML algorithms Implemented Pandas, NumPy, Seaborn, SciPy, Matplotlib, Scikit-Learn in Python to perform data cleaning and data visualization, processing techniques like checking the skewness and distributing the data normally by log transformation and Box-Cox"
            },
            {
               "job_title": "Data Scientist",
               "company": "Ulta Beauty",
               "duration_months": 22,
               "from": "Sep 2017",
               "to": "Jul 2019",
               "is_current": false,
               "work_description": "Worked independently and collaboratively throughout the analytical lifecycle including data extraction, data preparation, design and implementation of scalable analysis and solutions, and documentation of results Collaborated with technical/non-technical resources across the business to support and integrate our efforts and supported data management workflow from data collection, storage, analysis to training and validation Testing multiple classification model like random forest, SVM, logistic regression and gradient boosting. Also performed hyper-parameter tuning on the models to optimize the model predicting power in Python Experimented various DL algorithms and ensured that the model has low false positive rate Obtained knowledge of image processing algorithm: encoding/decoding, feature detection and matching, image segmentation and transformation Used a combination or various filter sizes for convolutions, max pooling, selection of activation functions (ReLu and SoftMax), dropout functions and batch normalization to regularize data Built a CNN model and NLP techniques to predict category to product using the label (title) of the image and modified the model to accommodate the CNN of the text blocks and integrated them both (image model and text model) to produce a reliable and highly accurate model Extensively used Databricks clusters and PySpark to extract and analyze text data with NLP techniques such as word-embedding and word similarities Involved in all jobs for pipelining the data from the databases through to analysis and reporting Simulated the model multiple times changing the hyperparameters such as learning rates, epochs, batch size, activation function, number of hidden layers and units, dropouts, and initial weights Developed machine learning/deep learning algorithms to classify the behavioral patterns using Machine Learning models such as random forests and decision trees on IDE like Vscode Studio, Pycharm. Developed machine learning strategies for risk analysis using multiple regression Worked with DevOps team to deploy applications on AWS EC2 and supported in automation testing Utilized AWS SageMaker to develop, train, and deploy machine learning models for efficiency and scalability of model training and tunning within the data pipeline Wrote complex SQL queries to extract and manipulate data from relational databases to ensure accurate and reliable data analysis Developed data models using SQL to ensure data integrity and optimize performance in data retrieval, supporting the machine learning pipeline and reporting efforts Created stored procedures with SQL to automate data extraction processes, enhancing efficiency in retrieving data for machine learning and analytical tasks"
            },
            {
               "job_title": "Data Scientist",
               "company": "Responsible for clarifying business objective",
               "duration_months": 15,
               "from": "May 2016",
               "to": "Aug 2017",
               "is_current": false,
               "work_description": "Responsible for clarifying business objective, data collection, data wrangling, data processing, machine learning modeling, model tunning, deploying models Worked closely with internal stakeholders such as business teams, product managers, engineering teams, and partners Collaborated with the data scientist team and BA team to analyze on building a predictive model based on the requirements using various ML algorithms Implemented Pandas, NumPy, Seaborn, SciPy, Matplotlib, Scikit-Learn in Python to perform data cleaning, processing techniques like checking the skewness and distributing the data normally by log transformation and Box-Cox Implemented various data processing techniques to manipulate the unstructured, and structured data and tackled highly imbalanced data set under sampling and over-sampling techniques like SMOTE Used Spark MLlib to leverage the computational power of Spark for data processing and building machine learning models to improve the performance and optimization for the large-scale dataset using Spark Context, Spark SQL, and Spark Data Frame Wrote PySpark queries to clean, impute and manipulate over 100 million records of customer data for EDA and modeling Connected the data sources such as AWS RDS and S3 to import data into QuickSight for data visualization and analyzing trends and summary statistics Built a multi-text classifier on the business data glossary to classify more than 3000 attributes using NLTK, Word2Vec to build word embedding Worked intensively on AWS Services like SageMaker, Lambda, EC2, EMR, and S3 for large-scale data storage, processing, and automation, to optimize resource usage for machine learning tasks Involved in performing text analytics with customer feedback in emails and call transcripts, built a text classifier and Sentiment Analyzer using Recurrent Network LSTM and a huge word cloud in Python Leverage AWS SageMaker to build, train, tune, and deploy state of art machine learning models and deep learning models Performed routine maintenance of data pipelines and machine learning models to address system errors and optimized workflows for efficiency and performance Created detailed and interactive reports for stakeholders by generating data visualizations with AWS QuickSight and performance summaries to present insights on model performance, resource utilization, and business impact for informed decision-making"
            }
         ],
         "total_experience_years": 9.09
      },
      "C:/Flexon_Resume_Parser/Parser_Build-Arnav/Test Resumes/Sample Resumes/Data Scientist_1.docx": {
         "name": "Data Scientist",
         "emails": [
            "email@mail.com"
         ],
         "phone_numbers": [
            "(123) 456-7890"
         ],
         "urls": [],
         "education": [
            "Postgraduate Certificate in Artificial Intelligence and Machine Learning - California Institute of Technology",
            "Associate of Science in Business Information System and Management - Canberra Institute of Technology, Australia",
            "Bachelor of Arts in English Language and Literature - University of Foreign Languages, Myanmar"
         ],
         "skills": [
            "Data Analyst",
            "Myanmar\tMay 2013 - Jun 2015",
            "Utilized SQL for data manipulation and analysis",
            "Philadelphia",
            "San Francisco",
            "transform",
            "WA\tJanuary 2014 - July 2016",
            "CA\tFebruary 2019 - November 2021",
            "Seattle",
            "NumPy",
            "Analyzed and optimized AI orchestration in Python",
            "PA\tAugust 2016 - January 2019",
            "Amazon Prime",
            "and continuously refined the model",
            "AlphaSense",
            "KBZ Bank",
            "Urban Outfitters",
            "Software Engineer",
            "Azure Data Factory",
            "Data Scientist"
         ],
         "work_experiences": [
            {
               "job_title": "Unknown Title",
               "company": "Azure Data Factory",
               "duration_months": 44.1,
               "from": "Jan 2022",
               "to": "Present",
               "is_current": true,
               "work_description": "Implemented machine learning algorithms to enhance marketing by segmenting potential buyers or tenants based on preferences and behavior patterns. Developed data ingestion pipelines using Azure Data Factory to reliably extract, transform, and load geospatial data into Azure Data Lake Storage Gen 2. Utilized Databricks for preprocessing and transforming geospatial data with Python and SQL, ensuring high-quality datasets for analysis. Designed and executed spatial data analyses and visualizations using Python GIS libraries, providing actionable geographic insights. Optimized data storage and partitioning in Azure Data Lake Storage Gen 2 to improve performance and reduce latency for geospatial datasets. Enhanced data processing efficiency by integrating automated ETL workflows between Databricks, Azure Data Factory, and Azure Data Lake."
            },
            {
               "job_title": "Unknown Title",
               "company": "Azure Data Factory",
               "duration_months": 33,
               "from": "Feb 2019",
               "to": "Nov 2021",
               "is_current": false,
               "work_description": "Designed and developed an AI-powered chatbot using large language models on Azure to enhance customer service, tailored for financial advisory needs. Utilized Azure Machine Learning and Python to build and optimize chatbot machine learning models, integrating natural language processing techniques. Deployed and fine-tuned the chatbot on Azure Cloud, leveraging Azure Cognitive Services for scalability and enhanced performance. Analyzed and optimized AI orchestration in Python, reducing dependencies and improving chatbot performance. Integrated NoSQL databases to manage the chatbot’s knowledge base, enhancing the accuracy and relevance of the financial advice provided. Collaborated with cross-functional teams and monitored chatbot performance, using insights to continuously refine and align the chatbot’s functionalities with business goals."
            },
            {
               "job_title": "Unknown Title",
               "company": "Data Scientist",
               "duration_months": 29,
               "from": "Aug 2016",
               "to": "Jan 2019",
               "is_current": false,
               "work_description": "Leveraged machine learning to refine the loyalty program by identifying customer segments based on preferences and spending patterns using unsupervised clustering algorithms like K-means. Conducted extensive data analysis with Python and libraries like scikit-learn, NumPy, and pandas to derive actionable insights from customer data. Designed personalized marketing campaigns using segmentation insights to tailor promotions and rewards, significantly increasing customer engagement and loyalty. Collaborated with marketing and product teams to integrate segmentation insights into broader business strategies, aligning with corporate goals and enhancing customer-centric initiatives. Utilized Docker to standardize the machine learning environment across teams, facilitating consistent testing and development of the loyalty program. Achieved a 25% increase in customer engagement through targeted promotions, documented processes, and continuously refined the model, enhancing the program’s scalability and supporting business growth."
            },
            {
               "job_title": "Unknown Title",
               "company": "Amazon Prime",
               "duration_months": 30,
               "from": "Jan 2014",
               "to": "Jul 2016",
               "is_current": false,
               "work_description": "Conducted exploratory data analysis (EDA) with Python to identify trends and anomalies in membership data, providing insights into growth and retention strategies. Utilized SQL for data manipulation and analysis, ensuring quality and consistency across large datasets from various sources. Developed predictive models for membership growth using machine learning techniques in Python, focusing on model accuracy and performance. Employed causal inference methods to assess the impact of marketing campaigns on membership sign-ups, offering data-driven optimization recommendations. Created complex systems models to simulate membership growth scenarios, aiding stakeholders in understanding potential outcomes for informed decision-making. Increased membership growth by 20% through targeted marketing strategies and campaigns, leveraging advanced analytical insights and interactive data visualizations in Tableau for executive decision-making."
            },
            {
               "job_title": "Unknown Title",
               "company": "Software Engineer",
               "duration_months": 25,
               "from": "May 2013",
               "to": "Jun 2015",
               "is_current": false,
               "work_description": "Engineered a scalable Python API to fetch and deliver real-time currency conversion rates from multiple financial data providers, focusing on high availability and minimal latency. Implemented cloud infrastructure solutions to optimize the API's scalability and reliability, accommodating high-volume global requests. Designed a subscription feature within the API for rate alerts, enabling notifications via webhooks or emails when specific currency thresholds are reached. Enhanced user engagement by 40% by introducing features for multi-currency conversion and access to historical exchange rate data. Developed API endpoints for historical data retrieval, allowing comprehensive financial analysis and trend identification in a single API call. Collaborated with financial analysts to continuously refine API functionalities, ensuring alignment with user needs and market demands, and provided interactive documentation to aid user efficiency."
            }
         ],
         "total_experience_years": 13.42
      }
   }
}