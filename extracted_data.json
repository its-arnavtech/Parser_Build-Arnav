{
   "pdf": {},
   "docx": {
      "C:/Flexon_Resume_Parser/Parser_Build-Arnav/Test Resumes/Sample Resumes/AI Engineer.docx": {
         "name": "Bert",
         "emails": [
            "email@mail.com"
         ],
         "phone_numbers": [
            "(123) 456-7890"
         ],
         "urls": [],
         "education": [
            "University of Texas at Dallas - Master of Science in Business Analytics;"
         ],
         "skills": [
            "Built scalable",
            "AWS CloudWatch",
            "algorithms like Random Forest",
            "AWS Glue",
            "Power BI",
            "relevancy ranking",
            "Machine Learning",
            "Prophet",
            "Data Pipeline",
            "and data anomalies",
            "PySpark",
            "system performance",
            "Apache Kafka",
            "Python",
            "and LSTM",
            "facilitating automated",
            "Dallas",
            "historical pricing trends",
            "Bert",
            "on Amazon SageMaker",
            "Environment: AWS Kinesis",
            "flight route popularity",
            "and LangChain",
            "LLM (Llama",
            "AWS S3",
            "Snowflake",
            "Southwest Airlines",
            "AWS Lambda",
            "Google Dialogflow",
            "and user query resolution",
            "genism)",
            "Google BARD",
            "and many more"
         ],
         "work_experiences": [
            {
               "job_title": "Unknown Title",
               "company": "Large Language Models",
               "duration_months": 21.5,
               "from": "Nov 2023",
               "to": "Present",
               "is_current": true,
               "work_description": "NerdWallet, Remote. Designed advanced Generative AI (GEN AI) powered applications using Large Language Models (LLMs), NLP, Google Dialogflow, and Retrieval-Augmented Generation (RAG), improving customer interaction experiences by providing contextually relevant and accurate responses. Engineered end-to-end machine learning pipelines, integrating GPT (LLM) for natural language processing (NLP), natural language understanding (NLU), relevancy, and ranking in document retrieval, enhancing automated support and content generation workflows. Led the PoC of a scalable GPT (LLM)-based chatbot using Decoder Transformer architecture, Google Dialogflow, and LangChain, facilitating automated, human-like responses for customer support. Built and maintained real-time data pipelines using Apache Kafka and Google Cloud Pub/Sub, enabling low-latency streaming for machine learning models and improving response times for customer queries. Implemented document retrieval systems with NLP-based query expansion and semantic search, improving the relevancy and ranking of search results for customer support applications powered by Google Dialogflow for enhanced user interaction. Applied NLP techniques to analyze customer feedback and reviews, extracting key insights using sentiment analysis and topic modeling, helping product teams prioritize feature enhancements. Implemented Google AI Platform for training and deploying machine learning models, ensuring efficient handling of large datasets and automated scaling for prediction services, reducing model deployment time and enhancing relevancy in predictions. Applied RAG techniques, combining traditional machine learning and information retrieval systems, improving the ranking, relevancy, and accuracy of customer query resolution through Google Dialogflow. Collaborated with data science teams to integrate GPT-based and NLP models for content summarization, relevancy ranking, and user query resolution, significantly improving the product knowledgebase. Developed CI/CD pipelines using GitLab and Google Cloud Build to streamline the deployment of machine learning models, ensuring rapid iteration, versioning, and relevancy improvements across production environments. Enhanced model performance with distributed systems using Apache Spark on Google Cloud Dataproc, accelerating data processing times and enabling real-time model inference with accurate ranking. Collaborated with DevOps and SRE teams to ensure high availability and monitoring of AI systems, optimizing infrastructure using Google Kubernetes Engine (GKE) for orchestration and deployment. Integrated Google Identity Platform for secure, scalable user authentication in AI-driven customer support platforms using Google Dialogflow, enhancing data privacy and secure access to sensitive data. Environment: LLMs, RAG, Google Dialogflow, Hugging Face, LangChain, Python, Google AI Platform, Apache Kafka, Google Cloud Pub/Sub, Google Cloud Dataproc, Spark, GitLab, Google Kubernetes Engine, Docker, Google Identity Platform, ELK Stack, CI/CD"
            },
            {
               "job_title": "Unknown Title",
               "company": "DevOps",
               "duration_months": 46,
               "from": "Jan 2020",
               "to": "Nov 2023",
               "is_current": false,
               "work_description": "Honda Motor Company, Remote. Architected and implemented a scalable real-time data pipeline using AWS Kinesis, Lambda, and S3 to process and onboard telemetry data for real-time analytics, ensuring relevancy of streaming data analysis. Developed data onboarding and processing workflows leveraging Managed Streaming for Apache Kafka (MSK) and PySpark to stream, enrich, and transform vehicle telemetry data, integrating NLP algorithms for automated text extraction and ranking of insights based on data importance. Architected the cloud infrastructure using AWS CloudFormation (IAC) to automate the provisioning of resources like S3, EC2, RDS, Redshift, etc., optimizing for high relevancy in data flow and processing. Integrated Snowflake as the central data warehouse, optimizing storage and query performance through partitioned tables, SQL-based transformations, and NLP-driven data categorization, ensuring faster retrieval and ranking of critical data. Collaborated with data scientists to design and deploy machine learning models using Python, PySpark, and SageMaker, applying ranking algorithms to prioritize vehicle component failure predictions and trigger proactive maintenance based on high-relevancy alerts. Applied AWS Glue Data Catalog to organize and manage metadata, utilizing NLP to classify and rank data for efficient access and analysis, ensuring relevant data is prioritized in workflows. Developed and maintained large-scale data pipelines using Apache Spark, MapReduce, and PySpark, with MSK handling big data processing, ensuring that high-relevancy data is ranked, for faster ETL workflows. Implemented supervised learning techniques, including Support Vector Machines (SVM) and Decision Trees, on Amazon SageMaker, with NLP techniques applied to classify and rank incoming data. Leveraged Scikit-learn, TensorFlow, and PySpark for training machine learning models on Amazon SageMaker, including NLP models for real-time text extraction and ranking based on relevancy to vehicle performance and user behavior. Developed and deployed MLOps pipelines in production utilizing SageMaker, SageMaker-Data Wrangler, Elastic Kubernetes Service (EKS), and PySpark, incorporating NLP-driven ranking systems to enhance data quality and relevance for real-time analysis. Managed the end-to-end data pipeline lifecycle, from architecture design to deployment and optimization, ensuring high availability and reliability of PySpark-based pipelines, with ranking algorithms to prioritize data streams based on their relevancy for downstream processes. Developed custom ETL processes using AWS Glue, PySpark, and SQL to transform raw data into actionable insights, incorporating NLP and ranking models to improve downstream. Environment: AWS Kinesis, AWS Lambda, AWS S3, Apache Kafka, Snowflake, Python, AWS Glue, AWS CloudWatch, Power BI, Machine Learning, Data Pipeline, Terraform (Infrastructure as Code - IAC)"
            },
            {
               "job_title": "Unknown Title",
               "company": "SQL",
               "duration_months": 32,
               "from": "Mar 2017",
               "to": "Nov 2019",
               "is_current": false,
               "work_description": "Southwest Airlines, Dallas, TX"
            }
         ],
         "total_experience_years": 8.29
      },
      "C:/Flexon_Resume_Parser/Parser_Build-Arnav/Test Resumes/Sample Resumes/AI ML Engineer.docx": {
         "name": "ANN",
         "emails": [
            "email@mail.com"
         ],
         "phone_numbers": [
            "(123) 456-7890"
         ],
         "urls": [],
         "education": [
            "Masters in Data Science â€“ New Jersey Institute of Technology, Newark, NJ"
         ],
         "skills": [
            "Citibank",
            "Lowe's",
            "Chicago",
            "evaluating",
            "BERT",
            "ANN",
            "Proficient in Python",
            "KNN",
            "with a focus on Azure",
            "including Transformers",
            "and collaboration skills",
            "Expertise in Data Science",
            "Agentic",
            "specializing in ML",
            "and GPT models",
            "analysis",
            "optimizing model deployment",
            "scikit-learn",
            "scalability",
            "Newark",
            "Expertise in developing",
            "versioning",
            "LangChain",
            "Excellent problem-solving",
            "Mooresville",
            "querying",
            "communication",
            "NumPy",
            "New York",
            "Wells Fargo"
         ],
         "work_experiences": [
            {
               "job_title": "Data Scientist",
               "company": "Wells Fargo",
               "duration_months": 39.5,
               "from": "May 2022",
               "to": "Present",
               "is_current": true,
               "work_description": "Developed and deployed a generative AI-driven financial forecasting system using Python, SQL, and Azure ML, leveraging time series models such as ARIMA and Prophet for accurate market trend predictions. Implemented customer segmentation models using Python with KNN and Random Forest algorithms to categorize clients based on transaction behaviors, improving personalized marketing strategies. Built and fine-tuned Llama 2-based generative AI chatbots on Azure OpenAI using Python and ANN architectures to handle real-time customer queries and financial advice, increasing customer engagement and support efficiency. Utilized mlFlow to manage the end-to-end lifecycle of machine learning models, including tracking, versioning, and deploying predictive models for financial forecasting with KNN and ANN architectures. Integrated vector databases like Pinecone to store and retrieve high-dimensional embeddings, enabling fast, accurate responses from the generative AI chatbot and improving its adaptability to complex queries. Applied ANN and Transformer architectures for natural language processing tasks, enhancing chatbot accuracy and response time. Optimized time series models using Python for real-time forecasting of financial product demand, implementing mlFlow for model version control, and using advanced seasonal decomposition techniques. Monitored and evaluated model performance with mlFlow and Azure Machine Learning tools, continuously optimizing models with KNN and ANN based on performance metrics and business KPIs, ensuring reliable forecasting. Collaborated with cross-functional teams to integrate Python machine learning models into a scalable, cloud-based infrastructure on Azure, deploying ANN and KNN algorithms for diverse AI-powered services. Developed automated drift detection systems using Azure ML, ensuring continuous model adaptation and retraining to maintain high prediction accuracy, with mlFlow tracking for drift handling."
            },
            {
               "job_title": "Data Scientist",
               "company": "Python",
               "duration_months": 33,
               "from": "Jul 2019",
               "to": "Apr 2022",
               "is_current": false,
               "work_description": "Developed machine learning models using Python, employing popular libraries such as scikit-learn and pandas to automate the analysis of financial marketing content, enhancing customer engagement and improving campaign efficiency. Utilized SQL for efficient data extraction and preprocessing, ensuring the integrity and scalability of large financial datasets for training machine learning models in Python. Implemented ML models including Random Forest and KNN for customer segmentation, and leveraged Time Series Forecasting models in Python to predict key financial metrics such as demand fluctuations, customer behavior trends, and campaign performance. Designed and deployed Neural Network Architectures (ANN) and Transformers, incorporating BERT to analyze and generate financial content tailored to customer needs, enhancing personalized marketing strategies in financial services. Worked with Open Source Foundational Models like LLama2, customizing them for the financial domain, and integrated GPT models for generating financial advisory content and chatbot interactions that provide real-time, context-aware assistance to customers. Managed machine learning model life cycles using mlFlow, enabling seamless model tracking, versioning, and performance evaluation for KNN and ANN models, ensuring continuous improvements in model accuracy and deployment efficiency. Utilized Azure Machine Learning and Azure OpenAI for cloud-based ML services, optimizing model deployment, scalability, and integration with existing business systems in the cloud. Integrated Vector Databases to store and retrieve financial data in a high-dimensional space, enhancing the effectiveness of LLM/GEN-AI tools and frameworks like LangChain and Agentic for advanced query handling and content generation. Implemented performance evaluation mechanisms in mlFlow to assess model accuracy, scalability, and drift handling in both ML/DL systems and KNN/ANN models, ensuring deployed solutions remained responsive to evolving customer needs and financial trends."
            },
            {
               "job_title": "Data Analyst",
               "company": "ML/DL",
               "duration_months": 19,
               "from": "Dec 2017",
               "to": "Jul 2019",
               "is_current": false,
               "work_description": "Developed and implemented machine learning models using Python to enhance retail inventory management with a focus on demand forecasting and stock optimization, ensuring improved product availability and reducing stockouts. Leveraged advanced techniques to predict sales trends and manage inventory dynamically. Engineered automated ETL pipelines using Azure Data Factory, enabling real-time extraction, transformation, and loading (ETL) of sales and inventory data from multiple sources. This ensured data integrity and readiness for ML model training and evaluation. Applied advanced statistical techniques and machine learning algorithms, including Time Series Analysis, Random Forest, KNN, and Regression Models to enhance forecasting accuracy for inventory replenishment, improving operational efficiency and ensuring timely stock replenishment in retail settings. Developed interactive dashboards with Power BI to visualize critical KPIs related to inventory turnover, demand trends, and sales projections, supporting data-driven decision-making and optimized supply chain management in retail operations. Integrated machine learning models with retail systems using Azure Functions for serverless execution, providing real-time access to predictive insights and optimizing inventory management and supply chain operations, enhancing both operational efficiency and decision-making. Utilized Azure Machine Learning (Azure ML) for scalable, automated model training and deployment, managing the end-to-end machine learning lifecycle, from model development to real-time deployment in production environments. Ensured data security through Azure Active Directory (AAD) and Azure Key Vault, protecting sensitive retail data and ensuring compliance with industry regulations, thereby safeguarding both customer and business data. Monitored and fine-tuned model performance using Azure Monitor and Azure Application Insights, ensuring optimal resource utilization and quickly adapting to changing sales patterns and business needs, improving prediction accuracy and reliability over time. Leveraged Azure SQL Database for data storage, enabling high availability, scalability, and secure management of transactional and historical retail data, allowing efficient querying and reporting to optimize inventory. Collaborated with cross-functional teams, including data analysts, operations staff, and supply chain managers, ensuring alignment between machine learning initiatives and business goals. This collaboration optimized inventory levels and enhanced customer experience through improved product availability. Documented machine learning workflows and system architecture, providing comprehensive training and support for team members, ensuring seamless deployment and operational integration of machine learning solutions into retail operations."
            }
         ],
         "total_experience_years": 7.62
      },
      "C:/Flexon_Resume_Parser/Parser_Build-Arnav/Test Resumes/Sample Resumes/Cloud Engineer.docx": {
         "name": "Docker",
         "emails": [],
         "phone_numbers": [],
         "urls": [],
         "education": [
            "University of Texas at Dallas - Master of Science in Business Analytics;"
         ],
         "skills": [
            "Docker",
            "AWS EKS",
            "PostgreSQL",
            "CloudFormation",
            "AWS Glue",
            "CloudFormation (IaC)",
            "SQL",
            "and AWS SQS",
            "Jenkins",
            "transformation",
            "AWS Redshift",
            "scalability",
            "SNS",
            "Snowflake",
            "Southwest Airlines",
            "and external sources",
            "TensorFlow",
            "AWS SageMaker",
            "Environment: AWS S3",
            "NerdWallet",
            "security",
            "Amazon SNS",
            "AWS CloudWatch",
            "CloudTrail",
            "and Datadog",
            "Environment: AWS Lambda",
            "Bengaluru",
            "Torrence",
            "and storage optimization",
            "CloudWatch",
            "and PySpark to stream",
            "Lambda",
            "Amazon RDS",
            "Git",
            "enhancing scalability",
            "Amazon Cognito",
            "enrich telemetry data",
            "enabling scalable",
            "Bengalore",
            "AWS Kinesis",
            "and predictive maintenance",
            "and AWS CodePipeline",
            "Power BI",
            "RDS",
            "PySpark",
            "Apache Kafka",
            "Kubernetes",
            "Kubernetes (EKS)",
            "Redshift",
            "Glue",
            "and SQS",
            "ensuring reliability",
            "detect data drift",
            "Honda Motor Company",
            "analyze",
            "and Kubernetes",
            "and Amazon SQS",
            "GitHub",
            "Amazon Lex",
            "EC2",
            "RDS)",
            "Data Pipeline",
            "Environment: Python",
            "Python",
            "SQS",
            "Dallas",
            "testing",
            "including data ingestion",
            "Environment: AWS Kinesis",
            "failure points",
            "ETL",
            "operational data",
            "access control",
            "Terraform (IAC)",
            "Leveraged Scikit-learn",
            "CI/CD",
            "AWS Lambda",
            "streamlining model training",
            "achieving high availability",
            "IND",
            "Amazon Kinesis"
         ],
         "work_experiences": [],
         "total_experience_years": 0
      },
      "C:/Flexon_Resume_Parser/Parser_Build-Arnav/Test Resumes/Sample Resumes/Data Engineer.docx": {
         "name": "Docker",
         "emails": [
            "email@mail.com"
         ],
         "phone_numbers": [
            "(123) 456-7890"
         ],
         "urls": [],
         "education": [
            "University of Texas at Dallas - Master of Science in Business Analytics; Specialization in Applied Machine Learning."
         ],
         "skills": [
            "ensuring data encryption",
            "potential failure points",
            "and Kubernetes",
            "AWS CloudWatch",
            "CloudTrail",
            "including data cleaning",
            "and Datadog",
            "Datadog",
            "Docker",
            "AWS EKS",
            "normalization",
            "Environment: AWS Lambda",
            "Amazon Lex",
            "San Francisco",
            "AWS Glue",
            "EC2",
            "Power BI",
            "RDS",
            "and storage optimization",
            "RDS)",
            "Machine Learning",
            "CloudWatch",
            "Data Pipeline",
            "Apache Kafka",
            "Python",
            "SQL",
            "reducing response times",
            "testing",
            "including data ingestion",
            "Lambda",
            "Environment: AWS Kinesis",
            "transformation",
            "Redshift",
            "ETL",
            "operational data",
            "access control",
            "AWS Redshift",
            "Glue",
            "Amazon RDS",
            "Omaha",
            "AWS S3",
            "Snowflake",
            "and feature engineering",
            "Amazon Cognito",
            "and external sources",
            "AWS Lambda",
            "AWS SageMaker",
            "Environment: AWS S3",
            "streamlining model training",
            "detect data drift",
            "NerdWallet",
            "analyze",
            "Amazon Kinesis"
         ],
         "work_experiences": [
            {
               "job_title": "Unknown Title",
               "company": "Jun 2022",
               "duration_months": 38.5,
               "from": "Jun 2022",
               "to": "Present",
               "is_current": true,
               "work_description": "Honda Motor Company, Remote. Architected and implemented a scalable real-time data pipeline using AWS Kinesis, Lambda, and S3 to process telemetry data. Developed data ingestion and processing workflows leveraging Apache Kafka to stream and enrich vehicle telemetry data, ensuring real-time data availability. Integrated Snowflake as the central data warehouse, optimizing storage and query performance through partitioned tables for advanced analytics. Collaborated with data scientists to design and deploy machine learning models in Python that predict vehicle component failures, triggering proactive maintenance alerts. Established monitoring and log systems using AWS CloudWatch and Datadog, optimizing data flow to handle peak loads with consistent throughput and minimal latency. Worked with cross-functional teams, including software engineers and product managers, to define technical requirements and ensure seamless pipeline integration. Implemented data governance practices, ensuring data encryption, access control, and compliance with data privacy regulations across all telemetry data processes. Automated data pipeline workflows using AWS Glue and Lambda, reducing manual intervention and improving data freshness for real-time analytics. Set up a monitoring framework using Datadog to track data pipeline performance and flow, allowing for proactive issue detection and logging. Designed and delivered dashboards and reports in Power BI, providing insights into vehicle performance, potential failure points, and predictive maintenance effectiveness. Improved predictive maintenance accuracy, reducing unexpected vehicle breakdowns and enhancing customer satisfaction. Reduced overall maintenance costs through proactive identification and resolution of vehicle issues before they become critical. Achieved sub-second data processing latency, ensuring timely and accurate data for predictive analytics and decision-making. Managed the end-to-end data pipeline lifecycle, from initial architecture to deployment and ongoing optimization, ensuring high availability and reliability. Executed exploratory data analysis to uncover patterns and relationships in customer behavior, which guided feature selection and informed the development of more effective predictive models. Visualized prediction outcomes using Power BI, creating interactive dashboards and comprehensive reports to effectively communicate insights and predictions to key stakeholders. Delivered knowledge transfer sessions and detailed documentation to enable smooth handover of the solution to the operations and maintenance teams. Environment: AWS Kinesis, AWS Lambda, AWS S3, Apache Kafka, Snowflake, Python, AWS Glue, AWS CloudWatch, Power BI, Machine Learning, Data Pipeline, Datadog"
            },
            {
               "job_title": "Unknown Title",
               "company": "Power BI",
               "duration_months": 29,
               "from": "Jan 2020",
               "to": "Jun 2022",
               "is_current": false,
               "work_description": "Berkshire Hathaway Homestate Companies (BHHC), Omaha, NE Architected the cloud infrastructure on AWS using AWS CloudFormation to automate the provisioning of resources like S3, EC2, RDS, Redshift, etc. Designed and implemented scalable data pipelines that integrated AWS services (S3, RDS, Redshift, Glue) with Snowflake to enable seamless data processing and storage. Architected the end-to-end data flow from AWS S3 to Snowflake, ensuring efficient data ingestion, transformation, and loading processes while minimizing latency. Developed custom ETL processes using AWS Glue and SQL to transform raw data into actionable insights, optimizing data for downstream analytics. Performed extensive data preprocessing, including data cleaning, normalization, and feature engineering, to prepare large-scale historical data for accurate analysis and predictive modeling. Implemented robust error handling mechanisms, including retry logic and alerting, to ensure data pipeline reliability and quick resolution of data processing failures. Automated the deployment and management of data pipeline components using AWS CloudFormation, reducing setup time and ensuring consistency across environments. Integrated AWS Lambda functions to trigger real-time data processing workflows based on specific events, improving data timeliness for business-critical operations. Designed the security architecture with IAM roles and policies. Utilized AWS KMS for encryption to protect data in transit and at rest. Configured AWS Redshift as a temporary staging environment for large datasets, allowing for efficient data aggregation before final transfer to Snowflake. Conducted performance tuning of SQL queries and Snowflake data warehouse configurations to enhance query speed and reduce processing costs. Set up a monitoring framework using Datadog to track data pipeline performance and flow, allowing for proactive issue detection and logging. Optimized data partitioning and storage strategies in Snowflake, ensuring the pipeline could efficiently handle varying data volumes without performance degradation. Delivered detailed documentation and knowledge transfer sessions to ensure the smooth handover of the solution to the operations team. Achieved near-zero downtime with the implementation of robust error handling and automated recovery mechanisms. Environment: AWS Lambda, AWS Glue, Redshift, Snowflake, SQL, CloudWatch, ETL, Data Pipeline, CloudTrail, Datadog"
            },
            {
               "job_title": "Unknown Title",
               "company": "Redshift",
               "duration_months": 20,
               "from": "Mar 2018",
               "to": "Nov 2019",
               "is_current": false,
               "work_description": "NerdWallet, San Francisco, CA Designed and optimized end-to-end ETL pipelines using AWS services (S3, Glue, Redshift, RDS), ensuring automated T-1 data extraction, transformation, and loading of data. Engineered and implemented an AI-powered chat/voice bot using Amazon Lex to replace the legacy IVR system, automating customer support processes and leading to a 60% reduction in manual intervention. Created and managed data streaming pipelines using Amazon Kinesis, enabling the company to process and analyze customer behavior data in real time, allowing the company to respond quickly to changes in customer preferences and market trends. Conducted extensive data analysis on customer support interactions, leveraging machine learning to extract actionable insights and inform product update strategies. Enhanced ETL pipelines to extract data form Salesforce email campaign data, operational data, and external sources, achieving increase in performance efficiency. Developed and deployed machine learning models in production utilizing AWS SageMaker and EKS to enhance data quality, achieving an improvement in overall data utilization. Developed live dashboards for real-time issue tracking using Power BI, enabling the software development and product teams to monitor, analyze, and address the most frequent customer complaints. Collaborated with marketing and customer support teams to implement targeted retention strategies based on churn predictions, directly contributing to improvement in customer satisfaction. Performed extensive data preprocessing, including data cleaning, normalization, and feature engineering, to prepare large-scale historical data for accurate analysis and predictive modelling. Improved customer satisfaction by streamlining the support process, reducing response times, and providing the development team with insights to address common issues proactively. Implemented data security and privacy measures in compliance with industry regulations, ensuring data protection and governance. Established more secure customer profiles using Amazon Cognito, further improving customer privacy. Environment: AWS S3, AWS Glue, AWS Redshift, Amazon RDS, Amazon Lex, Amazon Kinesis, AWS SageMaker, AWS EKS, Python, Power BI, ETL, Amazon Cognito"
            }
         ],
         "total_experience_years": 7.29
      },
      "C:/Flexon_Resume_Parser/Parser_Build-Arnav/Test Resumes/Sample Resumes/Data Scientist AI_ML Engineer.docx": {
         "name": "Seaborn",
         "emails": [
            "email@mail.com"
         ],
         "phone_numbers": [
            "(123) 456-7890"
         ],
         "urls": [],
         "education": [],
         "skills": [
            "deploying models",
            "AUC-ROC curve",
            "activation function",
            "train",
            "predictive algorithms",
            "EC2",
            "epochs",
            "data collection",
            "batch size",
            "EMR",
            "Developed system models",
            "max pooling",
            "product managers",
            "and partners",
            "Lambda",
            "data wrangling",
            "tune",
            "Pandas",
            "PCA",
            "adjusted R square",
            "model tunning",
            "and initial weights",
            "and automation",
            "machine learning modeling",
            "confusion matrix",
            "dropouts"
         ],
         "work_experiences": [],
         "total_experience_years": 0
      },
      "C:/Flexon_Resume_Parser/Parser_Build-Arnav/Test Resumes/Sample Resumes/Data Scientist_1.docx": {
         "name": "Data Scientist",
         "emails": [
            "email@mail.com"
         ],
         "phone_numbers": [
            "(123) 456-7890"
         ],
         "urls": [],
         "education": [
            "Postgraduate Certificate in Artificial Intelligence and Machine Learning - California Institute of Technology",
            "Associate of Science in Business Information System and Management - Canberra Institute of Technology, Australia",
            "Bachelor of Arts in English Language and Literature - University of Foreign Languages, Myanmar"
         ],
         "skills": [
            "Myanmar\tMay 2013 - Jun 2015",
            "PA\tAugust 2016 - January 2019",
            "Azure Data Factory",
            "Utilized SQL for data manipulation and analysis",
            "Software Engineer",
            "Seattle",
            "Data Analyst",
            "AlphaSense",
            "San Francisco",
            "Amazon Prime",
            "Urban Outfitters",
            "Analyzed and optimized AI orchestration in Python",
            "Data Scientist",
            "KBZ Bank",
            "Philadelphia",
            "transform",
            "NumPy",
            "CA\tFebruary 2019 - November 2021",
            "and continuously refined the model",
            "WA\tJanuary 2014 - July 2016"
         ],
         "work_experiences": [
            {
               "job_title": "Unknown Title",
               "company": "CoStar Group - Washington D.C.\tJanuary 2022 - Present Implemented",
               "duration_months": 43.5,
               "from": "Jan 2022",
               "to": "Present",
               "is_current": true,
               "work_description": "Implemented machine learning algorithms to enhance marketing by segmenting potential buyers or tenants based on preferences and behavior patterns. Developed data ingestion pipelines using Azure Data Factory to reliably extract, transform, and load geospatial data into Azure Data Lake Storage Gen 2. Utilized Databricks for preprocessing and transforming geospatial data with Python and SQL, ensuring high-quality datasets for analysis. Designed and executed spatial data analyses and visualizations using Python GIS libraries, providing actionable geographic insights. Optimized data storage and partitioning in Azure Data Lake Storage Gen 2 to improve performance and reduce latency for geospatial datasets. Enhanced data processing efficiency by integrating automated ETL workflows between Databricks, Azure Data Factory, and Azure Data Lake."
            },
            {
               "job_title": "Unknown Title",
               "company": "GIS",
               "duration_months": 33,
               "from": "Feb 2019",
               "to": "Nov 2021",
               "is_current": false,
               "work_description": "Designed and developed an AI-powered chatbot using large language models on Azure to enhance customer service, tailored for financial advisory needs. Utilized Azure Machine Learning and Python to build and optimize chatbot machine learning models, integrating natural language processing techniques. Deployed and fine-tuned the chatbot on Azure Cloud, leveraging Azure Cognitive Services for scalability and enhanced performance. Analyzed and optimized AI orchestration in Python, reducing dependencies and improving chatbot performance. Integrated NoSQL databases to manage the chatbotâ€™s knowledge base, enhancing the accuracy and relevance of the financial advice provided. Collaborated with cross-functional teams and monitored chatbot performance, using insights to continuously refine and align the chatbotâ€™s functionalities with business goals."
            },
            {
               "job_title": "Unknown Title",
               "company": "Python",
               "duration_months": 29,
               "from": "Aug 2016",
               "to": "Jan 2019",
               "is_current": false,
               "work_description": "Leveraged machine learning to refine the loyalty program by identifying customer segments based on preferences and spending patterns using unsupervised clustering algorithms like K-means. Conducted extensive data analysis with Python and libraries like scikit-learn, NumPy, and pandas to derive actionable insights from customer data. Designed personalized marketing campaigns using segmentation insights to tailor promotions and rewards, significantly increasing customer engagement and loyalty. Collaborated with marketing and product teams to integrate segmentation insights into broader business strategies, aligning with corporate goals and enhancing customer-centric initiatives. Utilized Docker to standardize the machine learning environment across teams, facilitating consistent testing and development of the loyalty program. Achieved a 25% increase in customer engagement through targeted promotions, documented processes, and continuously refined the model, enhancing the programâ€™s scalability and supporting business growth."
            },
            {
               "job_title": "Unknown Title",
               "company": "Amazon Prime",
               "duration_months": 30,
               "from": "Jan 2014",
               "to": "Jul 2016",
               "is_current": false,
               "work_description": "Conducted exploratory data analysis (EDA) with Python to identify trends and anomalies in membership data, providing insights into growth and retention strategies. Utilized SQL for data manipulation and analysis, ensuring quality and consistency across large datasets from various sources. Developed predictive models for membership growth using machine learning techniques in Python, focusing on model accuracy and performance. Employed causal inference methods to assess the impact of marketing campaigns on membership sign-ups, offering data-driven optimization recommendations. Created complex systems models to simulate membership growth scenarios, aiding stakeholders in understanding potential outcomes for informed decision-making. Increased membership growth by 20% through targeted marketing strategies and campaigns, leveraging advanced analytical insights and interactive data visualizations in Tableau for executive decision-making."
            },
            {
               "job_title": "Unknown Title",
               "company": "Software Engineer",
               "duration_months": 25,
               "from": "May 2013",
               "to": "Jun 2015",
               "is_current": false,
               "work_description": "Engineered a scalable Python API to fetch and deliver real-time currency conversion rates from multiple financial data providers, focusing on high availability and minimal latency. Implemented cloud infrastructure solutions to optimize the API's scalability and reliability, accommodating high-volume global requests. Designed a subscription feature within the API for rate alerts, enabling notifications via webhooks or emails when specific currency thresholds are reached. Enhanced user engagement by 40% by introducing features for multi-currency conversion and access to historical exchange rate data. Developed API endpoints for historical data retrieval, allowing comprehensive financial analysis and trend identification in a single API call. Collaborated with financial analysts to continuously refine API functionalities, ensuring alignment with user needs and market demands, and provided interactive documentation to aid user efficiency."
            }
         ],
         "total_experience_years": 13.38
      }
   }
}